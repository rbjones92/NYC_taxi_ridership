{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfee4a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf9623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType, TimestampType\n",
    "from pyspark.sql.functions import col, count, when, isnan, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaa856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\Bob\\\\Desktop\\\\SpringBoard\\\\Python_Projects\\\\NYC_Taxi_Capstone\\\\New_York_Taxi_Data\\\\'\n",
    "\n",
    "for_hire_2015 = 'C:\\\\Users\\\\Bob\\\\Desktop\\\\SpringBoard\\\\Python_Projects\\\\NYC_Taxi_Capstone\\\\New_York_Taxi_Data\\\\For-Hire Vehicle Trip Records 2015-01.csv'\n",
    "\n",
    "for_hire_2017 = 'C:\\\\Users\\\\Bob\\\\Desktop\\\\SpringBoard\\\\Python_Projects\\\\NYC_Taxi_Capstone\\\\New_York_Taxi_Data\\\\For-Hire Vehicle Trip Records 2017-01.csv'\n",
    "\n",
    "for_hire_2018 = 'C:\\\\Users\\\\Bob\\\\Desktop\\\\SpringBoard\\\\Python_Projects\\\\NYC_Taxi_Capstone\\\\New_York_Taxi_Data\\\\For-Hire Vehicle Trip Records 2018-01.csv'\n",
    "\n",
    "for_hire_2019 = 'C:\\\\Users\\\\Bob\\\\Desktop\\\\SpringBoard\\\\Python_Projects\\\\NYC_Taxi_Capstone\\\\New_York_Taxi_Data\\\\For-Hire Vehicle Trip Records 2019-01.csv'\n",
    "\n",
    "for_hire_2020 = 'C:\\\\Users\\\\Bob\\\\Desktop\\\\SpringBoard\\\\Python_Projects\\\\NYC_Taxi_Capstone\\\\New_York_Taxi_Data\\\\For-Hire Vehicle Trip Records 2020-01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf78aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in column:\n",
      "+--------------------+-----------+----------+\n",
      "|Dispatching_base_num|Pickup_date|locationID|\n",
      "+--------------------+-----------+----------+\n",
      "|                   0|          0|    536455|\n",
      "+--------------------+-----------+----------+\n",
      "\n",
      "2746033 total rows\n",
      "3 total columns\n",
      "Schema:\n",
      "root\n",
      " |-- Dispatching_base_num: string (nullable = true)\n",
      " |-- Pickup_date: string (nullable = true)\n",
      " |-- locationID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Exploring data from (2015-2017) for hire records ####\n",
    "\n",
    "# Load\n",
    "df2015 = spark.read.format('csv').options(header=True).load(for_hire_2015)\n",
    "\n",
    "# Count nulls for each column \n",
    "print(\"Nulls in column:\")\n",
    "df2015.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2015.columns]).show()\n",
    "\n",
    "# Total rows \n",
    "print(df2015.count(),\"total rows\")\n",
    "\n",
    "# Total cols \n",
    "print(len(df2015.columns),\"total columns\")\n",
    "\n",
    "# Schema\n",
    "print(\"Schema:\")\n",
    "df2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa49e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in column:\n",
      "+--------------------+---------------+----------------+------------+------------+\n",
      "|Dispatching_base_num|Pickup_DateTime|DropOff_datetime|PUlocationID|DOlocationID|\n",
      "+--------------------+---------------+----------------+------------+------------+\n",
      "|                   0|              0|        13657210|     3704410|    13551199|\n",
      "+--------------------+---------------+----------------+------------+------------+\n",
      "\n",
      "13657212 total rows\n",
      "5 total columns\n",
      "Schema:\n",
      "root\n",
      " |-- Dispatching_base_num: string (nullable = true)\n",
      " |-- Pickup_DateTime: string (nullable = true)\n",
      " |-- DropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Exploring data from (2017-2018) for hire records ####\n",
    "\n",
    "# Load\n",
    "df2017 = spark.read.format('csv').options(header=True).load(for_hire_2017)\n",
    "\n",
    "# Count nulls for each column\n",
    "print(\"Nulls in column:\")\n",
    "df2017.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2017.columns]).show()\n",
    "\n",
    "# Total rows \n",
    "print(df2017.count(),\"total rows\")\n",
    "\n",
    "# Total cols \n",
    "print(len(df2017.columns),\"total columns\")\n",
    "\n",
    "# Schema\n",
    "print(\"Schema:\")\n",
    "df2017.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f2aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in column:\n",
      "+---------------+----------------+------------+------------+--------+-----------------------+--------------------+\n",
      "|Pickup_DateTime|DropOff_datetime|PUlocationID|DOlocationID| SR_Flag|Dispatching_base_number|Dispatching_base_num|\n",
      "+---------------+----------------+------------+------------+--------+-----------------------+--------------------+\n",
      "|              0|               0|     3569395|     1986916|15483286|                      0|            19808094|\n",
      "+---------------+----------------+------------+------------+--------+-----------------------+--------------------+\n",
      "\n",
      "19808094 total rows\n",
      "7 total columns\n",
      "Schema:\n",
      "root\n",
      " |-- Pickup_DateTime: string (nullable = true)\n",
      " |-- DropOff_datetime: string (nullable = true)\n",
      " |-- PUlocationID: string (nullable = true)\n",
      " |-- DOlocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Dispatching_base_number: string (nullable = true)\n",
      " |-- Dispatching_base_num: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Exploring data from (2018-2019) for hire records ####\n",
    "\n",
    "# Load\n",
    "df2018 = spark.read.format('csv').options(header=True).load(for_hire_2018)\n",
    "\n",
    "# Count nulls for each column\n",
    "print(\"Nulls in column:\")\n",
    "df2018.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2018.columns]).show()\n",
    "\n",
    "# Total rows \n",
    "print(df2018.count(),\"total rows\")\n",
    "\n",
    "# Total cols \n",
    "print(len(df2018.columns),\"total columns\")\n",
    "\n",
    "# Schema\n",
    "print(\"Schema:\")\n",
    "df2018.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b1e1b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in column\n",
      "+--------------------+---------------+----------------+------------+------------+--------+\n",
      "|dispatching_base_num|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID| SR_Flag|\n",
      "+--------------------+---------------+----------------+------------+------------+--------+\n",
      "|                   3|              0|               0|     1818836|      664275|17747181|\n",
      "+--------------------+---------------+----------------+------------+------------+--------+\n",
      "\n",
      "19808094 total rows\n",
      "6 total columns\n",
      "Schema:\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Exploring data from (2019-2020) for hire records ####\n",
    "\n",
    "# Load\n",
    "df2019 = spark.read.format('csv').options(header=True).load(for_hire_2019)\n",
    "\n",
    "# Count nulls for each column\n",
    "print(\"Nulls in column\")\n",
    "df2019.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2019.columns]).show()\n",
    "\n",
    "# Total rows \n",
    "print(df2018.count(),\"total rows\")\n",
    "\n",
    "# Total cols \n",
    "print(len(df2019.columns),\"total columns\")\n",
    "\n",
    "# Schema\n",
    "print(\"Schema:\")\n",
    "df2019.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0f0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in column:\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "|                   0|              4|               4|       56127|       20788|2028457|                 12722|\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "\n",
      "2028457 total rows\n",
      "7 total columns\n",
      "Schema:\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Exploring data from (2020-2021) for hire records ####\n",
    "\n",
    "# Load\n",
    "df2020 = spark.read.format('csv').options(header=True).load(for_hire_2020)\n",
    "\n",
    "# Count nulls for each column\n",
    "print(\"Nulls in column:\")\n",
    "df2020.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2020.columns]).show()\n",
    "\n",
    "# Total rows \n",
    "print(df2020.count(),\"total rows\")\n",
    "\n",
    "# Total cols \n",
    "print(len(df2020.columns),\"total columns\")\n",
    "\n",
    "# Schema\n",
    "print(\"Schema:\")\n",
    "df2020.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1104ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
