{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73b532d9-ee21-49bf-9e61-7aa9d1368505",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# New York Taxi ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95bf32d9-612d-4ca6-951f-c066f5fd25a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb6a826-68fc-4bd4-b6b1-f5b422735fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43969906-da7b-4de0-bd30-d5d1d1398fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Construct Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e072d6c0-2c7f-4c6e-abfc-7e3ea7cb8802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Start SparkSession with azure hadoop package \n",
    "spark = SparkSession.builder.master('local').appName('app').config('spark.jars.packages', 'org.apache.hadoop:hadoop-azure:3.3.1').getOrCreate()        \n",
    "spark.conf.set(\"fs.azure.account.key.springboardstorage.blob.core.windows.net\",\"aytLE9zNkSMkYuioLbflu5bhemJ6vMZ10hCKEMgSPURwFfZqBJpNOZjEJUsxirAhSOLZReYsuB2u+AStlBBQWw==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975b4271-d0ae-4658-abe7-dde2fb5a043d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o445.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/taxi_etl; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/taxi_etl\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1010)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1036)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:654)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:675)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:415)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:413)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:459)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:444)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:649)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:568)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:68)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:559)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:529)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:68)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:132)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1030)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/taxi_etl\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:535)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:903)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:684)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:892)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:543)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:368)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:421)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:413)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:465)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:450)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:368)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:324)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$3(ServerBackend.scala:147)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:144)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:565)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:660)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:681)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:421)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:413)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:465)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:450)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:655)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:574)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:565)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:535)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:137)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:968)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:879)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$3(JettyServer.scala:507)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$3$adapted(JettyServer.scala:482)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:389)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:421)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:419)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:413)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:389)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:162)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:482)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:383)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:81)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:73)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:70)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:76)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n\nAlready mounted\n"
     ]
    }
   ],
   "source": [
    "# Create mount mount to connect to azure blob\n",
    "# ...Use this once or enter into try / expect block\n",
    "try:\n",
    "    dbutils.fs.mount(source = \"wasbs://springboardcontainer@springboardstorage.blob.core.windows.net\",\n",
    "    mount_point = \"/mnt/taxi_etl\",\n",
    "    extra_configs = {\"fs.azure.account.key.springboardstorage.blob.core.windows.net\": \"aytLE9zNkSMkYuioLbflu5bhemJ6vMZ10hCKEMgSPURwFfZqBJpNOZjEJUsxirAhSOLZReYsuB2u+AStlBBQWw==\"})\n",
    "# How to pass in java.lang.IllegalArgumentException?\n",
    "except Exception as e:\n",
    "    print('Already mounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e5a310-21b6-49c1-8fd4-5303a1f2f0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View files in SpringBoard Container\n",
    "dbutils.fs.ls(\"/mnt/taxi_etl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7c2da7-c9e7-4d7f-8ffb-fec31d592f66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grab Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77d5c528-19fb-4234-bd41-2a82739a7fdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Find Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976bc8a6-9c0a-4a44-a030-83c43796eaa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example URLs\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_dates():\n",
    "    yellow_dates = pd.date_range('2011-01-01','2022-12-01',freq='MS').strftime(\"%Y-%m\").to_list()\n",
    "    # green_dates = pd.date_range('2014-01-01','2022-12-01',freq='MS').strftime(\"%Y-%m\").to_list()\n",
    "    # fh_dates = pd.date_range('2015-01-01','2022-12-01',freq='MS').strftime(\"%Y-%m\").to_list()\n",
    "    # hv_dates = pd.date_range('2019-02-01','2022-12-01',freq='MS').strftime(\"%Y-%m\").to_list()\n",
    "    return yellow_dates\n",
    "    \n",
    "yellow_dates = get_dates()[0]\n",
    "# green_dates = get_dates()[1]\n",
    "# fh_dates = get_dates()[2]\n",
    "# hv_dates = get_dates()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c54de9-3366-4548-af2a-8e0678145120",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions To Download and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac5c492-7b0f-4322-9b26-93a78c2bc469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ScrapeNyTaxi:\n",
    "    '''\n",
    "    Functions to loop thru select dates from get_dates() and download each parquet file in that range\n",
    "    Yellow taxi data: 2011-2022\n",
    "    Green taxi data: 2013-2022\n",
    "    For-hire vehicle data: 2015-2022\n",
    "    High-volume for-hire vehicle data: 2019-2022\n",
    "    '''\n",
    "    def grab_yellow():\n",
    "        '''\n",
    "        Download yellow cab parquet files \n",
    "        '''\n",
    "        for date in yellow_dates:\n",
    "\n",
    "            try:\n",
    "                url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{date}.parquet'\n",
    "                response = requests.get(url, allow_redirects=True)\n",
    "                open(f'/dbfs/mnt/taxi_etl/trip_data/yellow_{date}.parquet','wb').write(response.content)\n",
    "\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Exception: {e}')\n",
    "                continue\n",
    "    \n",
    "    '''\n",
    "    def grab_green():\n",
    "\n",
    "        # Download yellow cab parquet files \n",
    "\n",
    "        for date in green_dates:\n",
    "            try:\n",
    "                url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{date}.parquet'\n",
    "                response = requests.get(url, allow_redirects=True)\n",
    "                open(f'/dbfs/mnt/taxi_etl/trip_data/green_{date}.parquet','wb').write(response.content)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Exception: {e}')\n",
    "                continue\n",
    "            \n",
    "\n",
    "    def grab_fh():\n",
    "\n",
    "        # Download for hire vehicle parquet files \n",
    "\n",
    "        for date in fh_dates:\n",
    "            try:\n",
    "                url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_{date}.parquet'\n",
    "                response = requests.get(url, allow_redirects=True)\n",
    "                open(f'/dbfs/mnt/taxi_etl/trip_data/for_hire_{date}.parquet','wb').write(response.content)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Exception: {e}')\n",
    "                continue     \n",
    "\n",
    "    \n",
    "    def grab_hv():\n",
    "        # Download for hire vehicle parquet files \n",
    "        for date in hv_dates:\n",
    "            try:\n",
    "                url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_{date}.parquet'\n",
    "                response = requests.get(url, allow_redirects=True)\n",
    "                open(f'/dbfs/mnt/taxi_etl/trip_data/high_volume_{date}.parquet','wb').write(response.content)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Exception: {e}')\n",
    "                continue     \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576c3271-997c-480b-9bdc-5c3e970cefc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ScrapeNyTaxi.grab_yellow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b589b73-3b3f-494a-99df-174828a1b32f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ScrapeNyTaxi.grab_green()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a5db917-5cca-490c-b78c-4e202adf08d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ScrapeNyTaxi.grab_fh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "053cc3d4-59e0-44b3-b086-1baa4c0ce301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ScrapeNyTaxi.grab_hv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca8adea3-8afd-49f5-aeae-ac2d32bd5b4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Remove Empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958f2fba-45ec-4bb3-8e73-9b5fdc94c430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 391 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Total number of files\n",
    "print(f'Total: {len(onlyfiles)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5c485f-e6ba-4f3c-acfd-784ca59337a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Several files are empty, as there was no data to pull from the web\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(path+file) < 250:\n",
    "      print(f'{file} has no data')\n",
    "      os.remove(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd3a027-3c2c-477e-8d97-728ca4b3881b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 391 files\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Total number of files\n",
    "print(f'Total: {len(onlyfiles)} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8182e5bf-9fe2-45d5-8b32-ba9783aad80b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Print Size of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc688cef-028c-4695-9201-147debf897bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yellow_bytes = 0\n",
    "# green_bytes = 0\n",
    "# fh_bytes = 0\n",
    "# hv_bytes = 0\n",
    "for file in onlyfiles:\n",
    "    if file.startswith('yellow'):\n",
    "        yellow_bytes += os.path.getsize(path+file)\n",
    "'''\n",
    "    if file.startswith('green'):\n",
    "        green_bytes += os.path.getsize(path+file)\n",
    "    if file.startswith('for'):\n",
    "        fh_bytes += os.path.getsize(path+file)      \n",
    "    if file.startswith('high'):\n",
    "        hv_bytes += os.path.getsize(path+file)              \n",
    "'''\n",
    "print(f'yellow taxi data totals {yellow_bytes/1000000000} gigs')\n",
    "\n",
    "# print(f'green taxi data totals {green_bytes/1000000000} gigs')\n",
    "# print(f'for-hire taxi data totals {fh_bytes/1000000000} gigs')\n",
    "# print(f'high-volume taxi data totals {hv_bytes/1000000000} gigs')\n",
    "\n",
    "print(f'total data: {(green_bytes+yellow_bytes+fh_bytes+hv_bytes)/1000000000} gigs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "918723a9-f094-4e6c-8dc7-1b7a9848487d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explore Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032f7faa-d494-4dd9-aa3a-4b87002669bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce56e83-4086-404d-8d5a-b85e55690c27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "\n",
    "# for_hire = []\n",
    "# green_taxi = []\n",
    "# high_volume = []\n",
    "yellow_taxi = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.startswith('yellow'):\n",
    "        yellow_taxi.append(file)\n",
    "\n",
    "    '''\n",
    "    if file.startswith('for'):\n",
    "        for_hire.append(file)\n",
    "    if file.startswith('green'):\n",
    "        green_taxi.append(file)\n",
    "    if file.startswith('high'):\n",
    "        high_volume.append(file)\n",
    "    '''\n",
    "\n",
    "all_data = [yellow_taxi]\n",
    "# all_data = [for_hire,green_taxi,high_volume,yellow_taxi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa207e11-fcee-48ca-916b-fc91dae1b9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema(data):\n",
    "\n",
    "    file_list = []\n",
    "    schema_list = []\n",
    "\n",
    "    for files in data:\n",
    "        df = spark.read.option('inferSchema','true').format('parquet').load(directory[5:]+files)\n",
    "        file_list.append(files)\n",
    "        schema_list.append(str(df.dtypes))\n",
    "\n",
    "    list_zip = zip(file_list,schema_list)\n",
    "    zipped_list = list(list_zip)\n",
    "\n",
    "    df_schema = StructType([ \\\n",
    "        StructField(\"File\",StringType(),True), \\\n",
    "        StructField(\"Schema\",StringType(),True),\n",
    "    ]) \n",
    "\n",
    "    df = spark.createDataFrame(zipped_list,schema= df_schema)\n",
    "    df = df.groupBy(\"Schema\").agg(F.collect_list('File'))\n",
    "    \n",
    "    data_str = data[0]\n",
    "    name = data_str.split(' ')[0]\n",
    "    \n",
    "    df.write.json(f'/dbfs/mnt/taxi_etl/trip_data/{name}_schema.json')\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca09ee5a-b3c9-4d66-863f-6d20daa44b75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run schema finder for groups (yellow,green,for-hire,high-volume) of data\n",
    "for data in all_data:\n",
    "    get_schema(data)\n",
    "\n",
    "# Our result shows us that there are several schemas for each of the datasets which must be normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa0a326-98b4-4771-9b61-42d2303331ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Normalize Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c59de3aa-5bb3-4e51-8b14-10ec7544b127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Define Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d864a2b-a4ad-4998-ae21-ddb5ef63aa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\ngreen_schema = StructType([\\n    StructField('VendorID', LongType(), True),\\n    StructField('lpep_pickup_datetime', TimestampType(), True),\\n    StructField('lpep_dropoff_datetime', TimestampType(), True),\\n    StructField('store_and_fwd_flag', StringType(), True),\\n    StructField('trip_distance', DoubleType(), True),\\n    StructField('fare_amount', DoubleType(), True),\\n    StructField('extra', DoubleType(), True),\\n    StructField('mta_tax', DoubleType(), True),\\n    StructField('tip_amount', DoubleType(), True),\\n    StructField('tolls_amount', DoubleType(), True),\\n    StructField('ehail_fee', LongType(), True),\\n    StructField('improvement_surcharge', DoubleType(), True),\\n    StructField('total_amount', DoubleType(), True),\\n    StructField('payment_type', LongType(), True),\\n    StructField('trip_type', DoubleType(), True),\\n    StructField('congestion_surcharge', DoubleType(), True),\\n    StructField('taxi_type', StringType(), True),\\n    ])\\n\\nfhv_schema = StructType([\\n    StructField('dispatching_base_num', StringType(), True),\\n    StructField('pickup_datetime', TimestampType(), True),\\n    StructField('dropOff_datetime', TimestampType(), True),\\n    StructField('PULocationID', DoubleType(), True),\\n    StructField('DOLocationID', DoubleType(), True),\\n    StructField('SR_Flag', StringType(), True),\\n    StructField('Affiliated_base_number', StringType(), True),\\n    StructField('taxi_type', StringType(), True),\\n    ])\\n\\nhv_schema = StructType([\\n    StructField('hvfhs_license_num', StringType(), True),\\n    StructField('dispatching_base_num', StringType(), True),\\n    StructField('originating_base_num', StringType(), True),\\n    StructField('request_datetime', TimestampType(), True),\\n    StructField('on_scene_datetime', TimestampType(), True),\\n    StructField('pickup_datetime', TimestampType(), True),\\n    StructField('dropoff_datetime', TimestampType(), True),\\n    StructField('PULocationID', LongType(), True),\\n    StructField('DOLocationID', LongType(), True),\\n    StructField('trip_miles', DoubleType(), True),\\n    StructField('trip_time', LongType(), True),\\n    StructField('base_passenger_fare', DoubleType(), True),\\n    StructField('tolls', DoubleType(), True),\\n    StructField('bcf', DoubleType(), True),\\n    StructField('sales_tax', DoubleType(), True),\\n    StructField('congestion_surcharge', DoubleType(), True),\\n    StructField('airport_fee', DoubleType(), True),\\n    StructField('tips', DoubleType(), True),\\n    StructField('driver_pay', DoubleType(), True),\\n    StructField('shared_request_flag', StringType(), True),\\n    StructField('shared_match_flag', StringType(), True),\\n    StructField('access_a_ride_flag', StringType(), True),\\n    StructField('wav_request_flag', StringType(), True),\\n    StructField('wav_match_flag', StringType(), True),\\n    StructField('taxi_type', StringType(), True),\\n    ])\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Schemas for each type of data \n",
    "\n",
    "yellow_schema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('pickup_datetime', TimestampType(), True),\n",
    "    StructField('dropoff_datetime', TimestampType(), True),\n",
    "    StructField('passenger_count', StringType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('airport_fee', IntegerType(), True),\n",
    "    StructField('taxi_type', StringType(), True),\n",
    "    ])\n",
    "\n",
    "'''\n",
    "green_schema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('lpep_pickup_datetime', TimestampType(), True),\n",
    "    StructField('lpep_dropoff_datetime', TimestampType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('ehail_fee', LongType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('trip_type', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('taxi_type', StringType(), True),\n",
    "    ])\n",
    "\n",
    "fhv_schema = StructType([\n",
    "    StructField('dispatching_base_num', StringType(), True),\n",
    "    StructField('pickup_datetime', TimestampType(), True),\n",
    "    StructField('dropOff_datetime', TimestampType(), True),\n",
    "    StructField('PULocationID', DoubleType(), True),\n",
    "    StructField('DOLocationID', DoubleType(), True),\n",
    "    StructField('SR_Flag', StringType(), True),\n",
    "    StructField('Affiliated_base_number', StringType(), True),\n",
    "    StructField('taxi_type', StringType(), True),\n",
    "    ])\n",
    "\n",
    "hv_schema = StructType([\n",
    "    StructField('hvfhs_license_num', StringType(), True),\n",
    "    StructField('dispatching_base_num', StringType(), True),\n",
    "    StructField('originating_base_num', StringType(), True),\n",
    "    StructField('request_datetime', TimestampType(), True),\n",
    "    StructField('on_scene_datetime', TimestampType(), True),\n",
    "    StructField('pickup_datetime', TimestampType(), True),\n",
    "    StructField('dropoff_datetime', TimestampType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('trip_miles', DoubleType(), True),\n",
    "    StructField('trip_time', LongType(), True),\n",
    "    StructField('base_passenger_fare', DoubleType(), True),\n",
    "    StructField('tolls', DoubleType(), True),\n",
    "    StructField('bcf', DoubleType(), True),\n",
    "    StructField('sales_tax', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('airport_fee', DoubleType(), True),\n",
    "    StructField('tips', DoubleType(), True),\n",
    "    StructField('driver_pay', DoubleType(), True),\n",
    "    StructField('shared_request_flag', StringType(), True),\n",
    "    StructField('shared_match_flag', StringType(), True),\n",
    "    StructField('access_a_ride_flag', StringType(), True),\n",
    "    StructField('wav_request_flag', StringType(), True),\n",
    "    StructField('wav_match_flag', StringType(), True),\n",
    "    StructField('taxi_type', StringType(), True),\n",
    "    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77afc5a6-ecc6-4573-bc32-68bf86b7eb39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Cast Each Group's Schema and Union to Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e002410-1a65-4296-b821-b66bdf3d8bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def make_yellow():\n",
    "\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    yellow_df = spark.createDataFrame(emptyRDD,schema=yellow_schema)\n",
    "\n",
    "    yellow_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('yellow'):\n",
    "            yellow_list.append(file)    \n",
    "\n",
    "    for file in yellow_list:\n",
    "        df_yellow = spark.read.option('inferSchema','true').parquet(f'{directory[5:]}{file}')\n",
    "        df_yellow = df_yellow.withColumn('taxi_type',lit('yellow'))\n",
    "        df_yellow = df_yellow.withColumnRenamed('tpep_pickup_datetime','pickup_datetime')\\\n",
    "            .withColumnRenamed('tpep_dropoff_datetime','dropoff_datetime')\n",
    "\n",
    "        df_yellow.createOrReplaceTempView('Cast')\n",
    "\n",
    "        df_yellow = spark.sql(\"SELECT BIGINT(VendorID),TIMESTAMP(pickup_datetime),\\\n",
    "            TIMESTAMP(dropoff_datetime),DOUBLE(passenger_count),DOUBLE(trip_distance),\\\n",
    "            BIGINT(RatecodeID),STRING(store_and_fwd_flag),BIGINT(PULocationID),BIGINT(DOLocationID),\\\n",
    "            BIGINT(payment_type),DOUBLE(fare_amount),DOUBLE(extra),DOUBLE(mta_tax),DOUBLE(tip_amount),\\\n",
    "            DOUBLE(tolls_amount),DOUBLE(improvement_surcharge),DOUBLE(total_amount),DOUBLE(congestion_surcharge),\\\n",
    "            DOUBLE(airport_fee),STRING(taxi_type) from Cast\")\n",
    "\n",
    "        yellow_df = df_yellow.union(yellow_df)\n",
    "        print(f'{file} analyzed')\n",
    "\n",
    "    yellow_df.printSchema()\n",
    "\n",
    "    return yellow_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cce100c-1745-4c69-8dab-5ec6d906de85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def make_green():\n",
    "\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    green_df = spark.createDataFrame(emptyRDD,schema=green_schema)\n",
    "\n",
    "    green_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('green'):\n",
    "            green_list.append(file)    \n",
    "\n",
    "    for file in green_list:    \n",
    "\n",
    "        df_green = spark.read.option('inferSchema','true').parquet(f'{directory[5:]}{file}')\n",
    "        df_green = df_green.withColumnRenamed('lpep_pickup_datetime','pickup_datetime')\\\n",
    "            .withColumnRenamed('lpep_dropoff_datetime','dropoff_datetime')\n",
    "        df_green = df_green.withColumn('taxi_type',lit('green'))\n",
    "\n",
    "        df_green.createOrReplaceTempView('Cast')\n",
    "\n",
    "        df_green = spark.sql(\"SELECT BIGINT(VendorID),TIMESTAMP(pickup_datetime),\\\n",
    "            TIMESTAMP(dropoff_datetime),STRING(store_and_fwd_flag),DOUBLE(trip_distance),\\\n",
    "            DOUBLE(fare_amount),DOUBLE(extra),DOUBLE(mta_tax),DOUBLE(tip_amount),\\\n",
    "            DOUBLE(tolls_amount),BIGINT(ehail_fee),DOUBLE(improvement_surcharge),DOUBLE(total_amount),\\\n",
    "            BIGINT(payment_type),DOUBLE(trip_type),DOUBLE(congestion_surcharge),STRING(taxi_type) from Cast\")\n",
    "\n",
    "        green_df = df_green.union(green_df)\n",
    "        print(f'{file} analyzed')        \n",
    "\n",
    "    green_df.printSchema()\n",
    "\n",
    "    return green_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cb3fe79-dd42-43e9-baf7-8d3e6587b906",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def make_fhv():\n",
    "\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    fhv_df = spark.createDataFrame(emptyRDD,schema=fhv_schema)\n",
    "\n",
    "    fhv_list = []    \n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('for'):\n",
    "            fhv_list.append(file)    \n",
    "\n",
    "    for file in fhv_list:    \n",
    "\n",
    "        df_fhv = spark.read.option('inferSchema','true').parquet(f'{directory[5:]}{file}')\n",
    "        df_fhv = df_fhv.withColumn('taxi_type',lit('for_hire'))\n",
    "        df_fhv = df_fhv.withColumnRenamed('dropOff_datetime','dropoff_datetime')\n",
    "\n",
    "        df_fhv.createOrReplaceTempView('Cast')\n",
    "\n",
    "        df_fhv = spark.sql(\"SELECT STRING(dispatching_base_num),TIMESTAMP(pickup_datetime),\\\n",
    "            TIMESTAMP(dropoff_datetime),DOUBLE(PULocationID),DOUBLE(DOLocationID),STRING(SR_Flag),\\\n",
    "            STRING(Affiliated_base_number),STRING(taxi_type) from Cast\")\n",
    "\n",
    "        fhv_df = df_fhv.union(fhv_df)\n",
    "        print(f'{file} analyzed')        \n",
    "\n",
    "    fhv_df.printSchema()\n",
    "\n",
    "    return fhv_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4825c339-b9f2-474b-8405-433f7d95ba01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- hvfhs_license_num: string (nullable = true)\n |-- dispatching_base_num: string (nullable = true)\n |-- originating_base_num: string (nullable = true)\n |-- request_datetime: timestamp (nullable = true)\n |-- on_scene_datetime: timestamp (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- trip_miles: double (nullable = true)\n |-- trip_time: long (nullable = true)\n |-- base_passenger_fare: double (nullable = true)\n |-- tolls: double (nullable = true)\n |-- bcf: double (nullable = true)\n |-- sales_tax: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- tips: double (nullable = true)\n |-- driver_pay: double (nullable = true)\n |-- shared_request_flag: string (nullable = true)\n |-- shared_match_flag: string (nullable = true)\n |-- access_a_ride_flag: string (nullable = true)\n |-- wav_request_flag: string (nullable = true)\n |-- wav_match_flag: string (nullable = true)\n |-- taxi_type: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-317088730497197>:45\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;124;03m'''\u001B[39;00m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m    hv_list = []    \u001B[39;00m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    for file in os.listdir(directory):\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n",
       "\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hv_df\n",
       "\u001B[0;32m---> 45\u001B[0m make_hv()\n",
       "\n",
       "File \u001B[0;32m<command-317088730497197>:10\u001B[0m, in \u001B[0;36mmake_hv\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m hv_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mschema(hv_schema)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/mnt/taxi_etl/trip_data/high*.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      7\u001B[0m hv_df\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[0;32m---> 10\u001B[0m \u001B[43mhv_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/dbfs/mnt/taxi_etl/trip_data/all_hv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m### Try to read into one DF, using wildcard. If Schema fields are completely different, then you can split into separate DFs. Many unions will reshuffle and cause slowdowns. \u001B[39;00m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03mhv_list = []    \u001B[39;00m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mfor file in os.listdir(directory):\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1380\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n",
       "\u001B[1;32m   1378\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n",
       "\u001B[1;32m   1379\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n",
       "\u001B[0;32m-> 1380\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:209\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 209\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    210\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    211\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o585.parquet.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 171) (10.139.64.4 executor 3): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3312)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3244)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3235)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3235)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1425)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1425)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1425)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3524)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3462)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3450)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1170)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1158)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2720)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2703)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$5(FileFormatWriter.scala:380)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:347)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:377)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:266)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:150)\n",
       "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:206)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$3(commands.scala:128)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:125)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:140)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:252)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:424)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:190)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1035)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:144)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:374)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:252)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:237)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:250)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:243)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:243)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:243)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:197)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:188)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:300)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:897)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-317088730497197>:45\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03m    hv_list = []    \u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    for file in os.listdir(directory):\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m    \u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hv_df\n\u001B[0;32m---> 45\u001B[0m make_hv()\n\nFile \u001B[0;32m<command-317088730497197>:10\u001B[0m, in \u001B[0;36mmake_hv\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m hv_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mschema(hv_schema)\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/mnt/taxi_etl/trip_data/high*.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m hv_df\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[0;32m---> 10\u001B[0m \u001B[43mhv_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/dbfs/mnt/taxi_etl/trip_data/all_hv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m### Try to read into one DF, using wildcard. If Schema fields are completely different, then you can split into separate DFs. Many unions will reshuffle and cause slowdowns. \u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03mhv_list = []    \u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mfor file in os.listdir(directory):\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1380\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m   1379\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m-> 1380\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:209\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 209\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    211\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o585.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 171) (10.139.64.4 executor 3): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3312)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3244)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3235)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3235)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1425)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1425)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1425)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3524)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3462)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3450)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1170)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1158)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2720)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2703)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$5(FileFormatWriter.scala:380)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:347)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:377)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:266)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:150)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:206)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$3(commands.scala:128)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:126)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:125)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:140)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:424)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:190)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1035)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:374)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:237)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:250)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:243)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:243)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:197)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:188)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:300)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:250)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:897)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 171) (10.139.64.4 executor 3): java.lang.OutOfMemoryError: GC overhead limit exceeded",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "def make_hv():\n",
    "  \n",
    "    # emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    # hv_df = spark.createDataFrame(emptyRDD,schema=hv_schema)\n",
    "    hv_df = spark.read.schema(hv_schema).parquet('/mnt/taxi_etl/trip_data/high*.parquet')\n",
    "    hv_df.printSchema()\n",
    "    hv_df.write.mode('overwrite').parquet('/dbfs/mnt/taxi_etl/trip_data/all_hv')\n",
    "    \n",
    "    \n",
    "    ### Try to read into one DF, using wildcard. If Schema fields are completely different, then you can split into separate DFs. Many unions will reshuffle and cause slowdowns. \n",
    "    \n",
    "\n",
    "    hv_list = []    \n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('high'):\n",
    "            hv_list.append(file) \n",
    "\n",
    "    for file in hv_list:    \n",
    "\n",
    "        df_hv = spark.read.option('inferSchema','true').parquet(f'{directory[5:]}{file}')\n",
    "        df_hv = df_hv.withColumn('taxi_type',lit('high_volume'))\n",
    "        df_hv = df_hv.withColumnRenamed('shared_request_flag','SR_Flag')\n",
    "\n",
    "        df_hv.createOrReplaceTempView('Cast')\n",
    "\n",
    "        df_hv = spark.sql(\"SELECT STRING(hvfhs_license_num),STRING(dispatching_base_num),STRING(originating_base_num),TIMESTAMP(request_datetime),\\\n",
    "            TIMESTAMP(on_scene_datetime),TIMESTAMP(pickup_datetime),TIMESTAMP(dropoff_datetime),\\\n",
    "            DOUBLE(PULocationID),DOUBLE(DOLocationID),DOUBLE(trip_miles),DOUBLE(trip_time),\\\n",
    "            DOUBLE(base_passenger_fare),DOUBLE(tolls),DOUBLE(bcf),DOUBLE(sales_tax),DOUBLE(congestion_surcharge),\\\n",
    "            DOUBLE(airport_fee),DOUBLE(tips),DOUBLE(driver_pay),STRING(SR_Flag),STRING(shared_match_flag),\\\n",
    "            STRING(access_a_ride_flag),STRING(wav_request_flag),STRING(wav_match_flag),STRING(taxi_type) from Cast\")\n",
    "\n",
    "        hv_df = df_hv.union(hv_df)\n",
    "        print(f'{file} analyzed')        \n",
    "\n",
    "    hv_df.printSchema()\n",
    "    \n",
    "\n",
    "\n",
    "    return hv_df\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76da95d-1a0d-4b09-ad24-e935f4a39356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|       2| 2011-01-01 00:10:00|  2011-01-01 00:12:00|              4|          0.0|         1|              null|         145|         145|           1|        2.9|  0.5|    0.5|      0.28|         0.0|                  0.0|        4.18|                null|       null|\n|       2| 2011-01-01 00:04:00|  2011-01-01 00:13:00|              4|          0.0|         1|              null|         264|         264|           1|        5.7|  0.5|    0.5|      0.24|         0.0|                  0.0|        6.94|                null|       null|\n|       2| 2011-01-01 00:14:00|  2011-01-01 00:16:00|              4|          0.0|         1|              null|         264|         264|           1|        2.9|  0.5|    0.5|      1.11|         0.0|                  0.0|        5.01|                null|       null|\n|       2| 2011-01-01 00:04:00|  2011-01-01 00:06:00|              5|          0.0|         1|              null|         146|         146|           1|        2.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.9|                null|       null|\n|       2| 2011-01-01 00:08:00|  2011-01-01 00:08:00|              5|          0.0|         1|              null|         146|         146|           1|        2.5|  0.5|    0.5|      0.11|         0.0|                  0.0|        3.61|                null|       null|\n|       2| 2011-01-01 00:23:00|  2011-01-01 00:23:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n|       2| 2011-01-01 00:25:00|  2011-01-01 00:25:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n|       1| 2011-01-01 00:58:10|  2011-01-01 01:15:35|              1|          8.0|         1|                 N|         138|         256|           2|       20.1|  0.5|    0.5|       0.0|         0.0|                  0.0|        21.1|                null|       null|\n|       1| 2011-01-01 00:23:27|  2011-01-01 00:39:39|              1|          1.6|         1|                 N|         170|         237|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|\n|       1| 2011-01-01 00:42:08|  2011-01-01 00:51:50|              4|          2.5|         1|                 N|         237|         170|           2|        8.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.1|                null|       null|\n|       1| 2011-01-01 00:53:36|  2011-01-01 01:17:43|              2|          3.9|         1|                 N|         170|         239|           1|       14.9|  0.5|    0.5|      2.38|         0.0|                  0.0|       18.28|                null|       null|\n|       1| 2011-01-01 00:37:47|  2011-01-01 00:41:20|              2|          0.6|         1|                 N|          90|          90|           2|        4.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.1|                null|       null|\n|       1| 2011-01-01 00:42:49|  2011-01-01 00:52:00|              4|          0.9|         1|                 N|          90|         186|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.5|                null|       null|\n|       1| 2011-01-01 00:56:28|  2011-01-01 01:22:36|              1|          3.9|         1|                 Y|          90|         238|           2|       15.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        16.3|                null|       null|\n|       1| 2011-01-01 00:11:22|  2011-01-01 00:14:36|              2|          0.7|         1|                 N|         113|          79|           2|        4.1|  0.0|    0.5|       0.0|         0.0|                  0.0|         4.6|                null|       null|\n|       1| 2011-01-01 00:16:43|  2011-01-01 00:24:39|              2|          1.9|         1|                 N|          79|         170|           1|        6.9|  0.0|    0.5|       1.0|         0.0|                  0.0|         8.4|                null|       null|\n|       1| 2011-01-01 00:32:25|  2011-01-01 00:48:46|              2|          3.3|         1|                 N|         170|         142|           2|       11.3|  0.0|    0.5|       0.0|         0.0|                  0.0|        11.8|                null|       null|\n|       1| 2011-01-01 00:50:52|  2011-01-01 01:25:47|              2|          5.3|         1|                 Y|         142|         112|           2|       20.1|  0.0|    0.5|       0.0|         4.8|                  0.0|        25.4|                null|       null|\n|       1| 2011-01-01 00:20:22|  2011-01-01 00:26:13|              1|          1.0|         4|                 N|         114|         249|           2|        5.3|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|\n|       1| 2011-01-01 00:28:45|  2011-01-01 00:46:14|              1|          4.3|         4|                 N|         249|         143|           2|       13.7|  0.5|    0.5|       0.0|         0.0|                  0.0|        14.7|                null|       null|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TOO SLOW, TIMES OUT ### \n",
    "\n",
    "# Storing too much in memory? If I write immedaitely after creating the DF, will this solve the issue? \n",
    "\n",
    "def write_dfs():\n",
    "\n",
    "    yellow_df = spark.read.schema(yellow_schema).format('parquet').load(directory[5:]+'yellow*.parquet')\n",
    "\n",
    "    # yellow_df = spark.read.option('inferSchema','true').format('parquet').load(directory[5:]+'yellow*.parquet')\n",
    "\n",
    "    yellow_df.show()\n",
    "\n",
    "    # yellow_df = make_yellow()\n",
    "    # green_df = make_green()\n",
    "    # fh_df = make_fhv()\n",
    "    # hv_df = make_hv()\n",
    "\n",
    "    # ERROR ON WRITE (DON'T NEED TO WRITE TO SINGLE DF?)\n",
    "    '''\n",
    "    org.apache.spark.SparkException: Job aborted due to stage failure: Task 84 in stage 12.0 failed 4 times, most recent failure: Lost task 84.3 in stage 12.0 (TID 590) (10.139.64.4 executor 0): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/taxi_etl/trip_data/yellow_2018-10.parquet. Parquet column cannot be converted. Column: [passenger_count], Expected: LongType, Found: DOUBLE\n",
    "    '''\n",
    "    # yellow_df.write.mode('overwrite').parquet('/dbfs/mnt/taxi_etl/trip_data/all_yellow')\n",
    "\n",
    "\n",
    "    # hv_df.write.parquet('/dbfs/mnt/taxi_etl/trip_data/all_hv')\n",
    "    # fh_df.write.parquet('/dbfs/mnt/taxi_etl/trip_data/combined_dfs/all_fh')\n",
    "    # green_df.write.parquet('/dbfs/mnt/taxi_etl/trip_data/all_green')\n",
    "    \n",
    "write_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3e5704-56ed-437c-ac90-c59de2a925b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grab New York Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81f577f-c5bb-479c-a416-ebcd5596496a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edfe3bf-be7f-46e6-80bf-a62b70df1f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting selenium\n  Downloading selenium-4.9.1-py3-none-any.whl (6.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 36.6 MB/s eta 0:00:00\nCollecting trio-websocket~=0.9\n  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: urllib3[socks]<3,>=1.26 in /databricks/python3/lib/python3.10/site-packages (from selenium) (1.26.11)\nRequirement already satisfied: certifi>=2021.10.8 in /databricks/python3/lib/python3.10/site-packages (from selenium) (2022.9.14)\nCollecting trio~=0.17\n  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 384.9/384.9 kB 25.8 MB/s eta 0:00:00\nRequirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.3)\nCollecting sortedcontainers\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: attrs>=19.2.0 in /databricks/python3/lib/python3.10/site-packages (from trio~=0.17->selenium) (21.4.0)\nCollecting async-generator>=1.9\n  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\nCollecting exceptiongroup>=1.0.0rc9\n  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\nCollecting outcome\n  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\nCollecting sniffio\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nCollecting wsproto>=0.14\n  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\nCollecting PySocks!=1.5.7,<2.0,>=1.5.6\n  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\nCollecting h11<1,>=0.9.0\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 6.4 MB/s eta 0:00:00\nInstalling collected packages: sortedcontainers, sniffio, PySocks, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\nSuccessfully installed PySocks-1.7.1 async-generator-1.10 exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.9.1 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c55c894-20a3-4b30-a205-8e95e8cbe756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46224362-c2c1-4237-b189-dd614a540525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Install Chrome and ChromeDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c84c38-553a-44d0-b539-90cf7da501e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1045 bytes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks/scripts/selenium-install.sh</td><td>selenium-install.sh</td><td>1045</td><td>1683585238000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks/scripts/selenium-install.sh",
         "selenium-install.sh",
         1045,
         1683585238000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "dbutils.fs.put(\"/databricks/scripts/selenium-install.sh\",\"\"\"\n",
    "#!/bin/bash\n",
    "%sh\n",
    "LAST_VERSION=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2FLAST_CHANGE?alt=media\"\n",
    "VERSION=$(curl -s -S $LAST_VERSION)\n",
    "if [ -d $VERSION ] ; then\n",
    "  echo \"version already installed\"\n",
    "  exit\n",
    "fi\n",
    " \n",
    "rm -rf /tmp/chrome/$VERSION\n",
    "mkdir -p /tmp/chrome/$VERSION\n",
    " \n",
    "URL=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F$VERSION%2Fchrome-linux.zip?alt=media\"\n",
    "ZIP=\"${VERSION}-chrome-linux.zip\"\n",
    " \n",
    "curl -# $URL > /tmp/chrome/$ZIP\n",
    "unzip /tmp/chrome/$ZIP -d /tmp/chrome/$VERSION\n",
    " \n",
    "URL=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F$VERSION%2Fchromedriver_linux64.zip?alt=media\"\n",
    "ZIP=\"${VERSION}-chromedriver_linux64.zip\"\n",
    " \n",
    "curl -# $URL > /tmp/chrome/$ZIP\n",
    "unzip /tmp/chrome/$ZIP -d /tmp/chrome/$VERSION\n",
    " \n",
    "mkdir -p /tmp/chrome/chrome-user-data-dir\n",
    " \n",
    "rm -f /tmp/chrome/latest\n",
    "ln -s /tmp/chrome/$VERSION /tmp/chrome/latest\n",
    " \n",
    "# to avoid errors about missing libraries\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y libgbm-dev\n",
    "\"\"\", True)\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b28efa-9b8d-4570-bba4-21db81d677b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dbfs/databricks/scripts/selenium-install.sh: line 3: fg: no job control\n#=#=#                                                                         \r\r                                                                           0.1%\r###                                                                        4.9%\r########                                                                  11.1%\r#############                                                             18.1%\r##################                                                        25.2%\r#######################                                                   32.4%\r############################                                              39.8%\r#################################                                         47.1%\r######################################                                    54.1%\r###########################################                               60.8%\r###############################################                           66.3%\r####################################################                      73.5%\r#########################################################                 80.3%\r###############################################################           87.6%\r####################################################################      94.7%\r######################################################################## 100.0%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/chrome/1141045-chrome-linux.zip\n  inflating: /tmp/chrome/1141045/chrome-linux/MEIPreload/manifest.json  \n  inflating: /tmp/chrome/1141045/chrome-linux/MEIPreload/preloaded_data.pb  \n  inflating: /tmp/chrome/1141045/chrome-linux/chrome  \n  inflating: /tmp/chrome/1141045/chrome-linux/chrome-wrapper  \n  inflating: /tmp/chrome/1141045/chrome-linux/chrome_100_percent.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/chrome_200_percent.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/chrome_crashpad_handler  \n  inflating: /tmp/chrome/1141045/chrome-linux/chrome_sandbox  \n  inflating: /tmp/chrome/1141045/chrome-linux/icudtl.dat  \n  inflating: /tmp/chrome/1141045/chrome-linux/libEGL.so  \n  inflating: /tmp/chrome/1141045/chrome-linux/libGLESv2.so  \n  inflating: /tmp/chrome/1141045/chrome-linux/libvk_swiftshader.so  \n  inflating: /tmp/chrome/1141045/chrome-linux/libvulkan.so.1  \n  inflating: /tmp/chrome/1141045/chrome-linux/nacl_helper  \n  inflating: /tmp/chrome/1141045/chrome-linux/nacl_helper_bootstrap  \n  inflating: /tmp/chrome/1141045/chrome-linux/nacl_irt_x86_64.nexe  \n extracting: /tmp/chrome/1141045/chrome-linux/product_logo_48.png  \n  inflating: /tmp/chrome/1141045/chrome-linux/resources.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/v8_context_snapshot.bin  \n  inflating: /tmp/chrome/1141045/chrome-linux/vk_swiftshader_icd.json  \n  inflating: /tmp/chrome/1141045/chrome-linux/xdg-mime  \n  inflating: /tmp/chrome/1141045/chrome-linux/xdg-settings  \n   creating: /tmp/chrome/1141045/chrome-linux/ClearKeyCdm/\n   creating: /tmp/chrome/1141045/chrome-linux/ClearKeyCdm/_platform_specific/\n   creating: /tmp/chrome/1141045/chrome-linux/ClearKeyCdm/_platform_specific/linux_x64/\n  inflating: /tmp/chrome/1141045/chrome-linux/ClearKeyCdm/_platform_specific/linux_x64/libclearkeycdm.so  \n   creating: /tmp/chrome/1141045/chrome-linux/locales/\n  inflating: /tmp/chrome/1141045/chrome-linux/locales/de.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/bg.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ko.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/he.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sr.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/hr.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/af.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ar.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ro.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sv.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ar.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/vi.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/es-419.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/nl.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ro.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/es-419.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/it.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ru.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/pt-BR.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fr.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/th.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ko.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sk.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/es.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/nb.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/kn.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fa.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/de.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/el.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/hu.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/it.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/lv.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/bg.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/zh-TW.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/zh-CN.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/zh-CN.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/cs.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/el.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ms.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/en-GB.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ar-XB.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sl.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/kn.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/tr.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ur.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/uk.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/cs.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/da.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ru.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fi.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fil.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/pt-PT.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/pl.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/es.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/hi.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/nl.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/te.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/bn.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/he.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/en-GB.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/en-US.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/af.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ta.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sk.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/uk.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/gu.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/gu.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ca.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/am.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/mr.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ja.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sl.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fa.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/en-US.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ta.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/id.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ml.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/hr.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/en-XA.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/pl.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/en-XA.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/bn.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/am.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/pt-BR.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sv.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/lv.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/zh-TW.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ja.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ur.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/pt-PT.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ms.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/th.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/et.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/lt.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ca.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/tr.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sw.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sr.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fr.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/lt.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/hu.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/da.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fil.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ml.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/et.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/ar-XB.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/id.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/te.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/vi.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/nb.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/hi.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/fi.pak.info  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/sw.pak  \n  inflating: /tmp/chrome/1141045/chrome-linux/locales/mr.pak.info  \n   creating: /tmp/chrome/1141045/chrome-linux/resources/\n   creating: /tmp/chrome/1141045/chrome-linux/resources/inspector_overlay/\n  inflating: /tmp/chrome/1141045/chrome-linux/resources/inspector_overlay/main.js  \n  inflating: /tmp/chrome/1141045/chrome-linux/resources/inspector_overlay/inspector_overlay_resources.grd  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#=#=#                                                                         \r\r                                                                           0.2%\r######################################                                    53.6%\r######################################################################## 100.0%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/chrome/1141045-chromedriver_linux64.zip\n  inflating: /tmp/chrome/1141045/chromedriver_linux64/LICENSE.chromedriver  \n  inflating: /tmp/chrome/1141045/chromedriver_linux64/chromedriver  \nHit:1 https://repos.azul.com/zulu/deb stable InRelease\nHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\nHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\nHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nReading package lists...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W: https://repos.azul.com/zulu/deb/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\nBuilding dependency tree...\nReading state information...\nlibgbm-dev is already the newest version (22.2.5-0ubuntu0.1~22.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "/dbfs/databricks/scripts/selenium-install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63f0686-fbfc-4031-9fb3-2b70aec10f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "s = Service('/tmp/chrome/latest/chromedriver_linux64/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"/tmp/chrome/latest/chrome-linux/chrome\"\n",
    "options.add_argument('headless')\n",
    "options.add_argument('--disable-infobars')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--remote-debugging-port=9222')\n",
    "options.add_argument('--homedir=/tmp/chrome/chrome-user-data-dir')\n",
    "options.add_argument('--user-data-dir=/tmp/chrome/chrome-user-data-dir')\n",
    "prefs = {\"download.default_directory\":\"/tmp/chrome/chrome-user-data-di\",\n",
    "         \"download.prompt_for_download\":False\n",
    "}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "# driver = webdriver.Chrome(service=s, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37c473-b382-4197-877f-8a5eb3abfd83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Load Wunderground Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9563dc6-21d0-4ba3-bcc7-cf040c8112d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find range of dates matching with Taxi Trip data\n",
    "# Earliest = 01/01/2010\n",
    "# Latest = 01/01/2023\n",
    "def get_dates():\n",
    "    d1 = datetime.date(2010,1,1)\n",
    "    d2 = datetime.date(2023,1,1)\n",
    "    dd = [d1 + datetime.timedelta(days=x) for x in range((d2-d1).days + 1)]\n",
    "    date_list = []\n",
    "    for d in dd:\n",
    "        date_list.append(str(d))\n",
    "    return date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc50a38-70cd-49c3-aee0-9dd216ee6b18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to load wunderground data (without this it has no records to show)\n",
    "def render_page(url):\n",
    "    driver = webdriver.Chrome(service=s, options=options)    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    r = driver.page_source\n",
    "    driver.quit()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af48ef43-0154-48af-9b50-93efc0a230d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_transpose(data_list):\n",
    "    res_list = [[item.replace('%', '') for item in lst] for lst in data_list]\n",
    "    res_list = [[item.replace(u'\\xa0', u'') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°F','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°in','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°%','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°mph','') for item in lst] for lst in res_list]\n",
    "    final_list = [[item.replace('°','') for item in lst] for lst in res_list]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94df9798-cad6-428a-8316-b8314725df8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def set_schema(df):\n",
    "    # To Interger\n",
    "    df[[\"Temperature\",\"Dew_Point\", \"Humidity\",\"Wind_Speed\",\"Wind_Gust\"]] = df[[\"Temperature\",\"Dew_Point\", \"Humidity\",\"Wind_Speed\",\"Wind_Gust\"]].apply(pd.to_numeric)\n",
    "    df[['Pressure','Precipitation']] = df[['Pressure','Precipitation']].apply(pd.to_numeric)\n",
    "    # To DateTime\n",
    "    df['datetime'] = df['datetime'].apply(pd.to_datetime)\n",
    "    # To String\n",
    "    df[['Wind','Condition']] = df[['Wind','Condition']].applymap(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958101fd-76d4-4ad9-b70e-227e079bc881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scraper(page, dates):\n",
    "    # function to scrape wunderground\n",
    "    for d in dates:\n",
    "\n",
    "        url = str(str(page) + str(d))\n",
    "\n",
    "        r = render_page(url)\n",
    "\n",
    "        soup = BS(r, \"html.parser\")\n",
    "        container = soup.find('lib-city-history-observation')\n",
    "        check = container.find('tbody')\n",
    "\n",
    "        data = []\n",
    "        try:\n",
    "            for c in check.find_all('tr', class_='ng-star-inserted'):\n",
    "                for i in c.find_all('td', class_='ng-star-inserted'):\n",
    "                    trial = i.text\n",
    "                    trial = trial.strip('  ')\n",
    "                    data.append(trial)\n",
    "            \n",
    "            df_daily = []\n",
    "            cols = ['Time','Temperature','Dew_Point','Humidity','Wind','Wind_Speed','Wind_Gust','Pressure','Precipitation','Condition','Date']\n",
    "            for i in range(0,len(data),10):\n",
    "                snip_data = []\n",
    "                snip_data.append(data[i:i+10])\n",
    "                # Strip of Weird Characters\n",
    "                snip_data = list_transpose(snip_data)\n",
    "                snip_data[0].append(d)\n",
    "                df = pd.DataFrame(snip_data,columns=cols)\n",
    "                df['datetime'] = df['Date'] + ' ' + df['Time']\n",
    "                df = df.drop(['Date','Time'],axis=1) \n",
    "                # Set Schema\n",
    "                df = set_schema(df)\n",
    "                df_daily.append(df)\n",
    "\n",
    "            df_daily = pd.concat(df_daily)\n",
    "            path = 'C:/Users/Robert.Jones/OneDrive - Central Coast Energy Services, Inc/Desktop/Springboard/Capstone/wunderground/parquet_files'\n",
    "                        \n",
    "            df_daily.to_parquet(f'/dbfs/mnt/taxi_etl/weather_data/NY_Weather{d}.parquet')\n",
    "            \n",
    "        except AttributeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6855812-e54f-490d-8c44-5116806c3230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call Functions\n",
    "dates = get_dates()\n",
    "page = 'https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/'\n",
    "\n",
    "df = scraper(page, dates)\n",
    "# Wrote 4745 files\n",
    "# Size of 42M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819824f7-e492-40e7-aabe-c9657d5da0ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh du -h /dbfs/mnt/taxi_etl/weather_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e135e588-4928-4d1c-b27d-43fb0d26cc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh ls /dbfs/mnt/taxi_etl/weather_data/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed63171e-c590-48b9-9c50-f73f190b07bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View files in SpringBoard Container\n",
    "dbutils.fs.mkdirs(\"/mnt/taxi_etl/weather_data/combined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6933f502-e8cb-4e7f-8398-4269b8d1cd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- temp(f): long (nullable = true)\n |-- dew_point(f): long (nullable = true)\n |-- humidity(%): long (nullable = true)\n |-- wind_direction: string (nullable = true)\n |-- wind_speed(mph): long (nullable = true)\n |-- wind_gust(mph): long (nullable = true)\n |-- pressure(inHg): double (nullable = true)\n |-- precipitation(in): double (nullable = true)\n |-- condition: string (nullable = true)\n |-- datetime: timestamp_ntz (nullable = true)\n\n+-------+------------+-----------+--------------+---------------+--------------+--------------+-----------------+------------------+-------------------+\n|temp(f)|dew_point(f)|humidity(%)|wind_direction|wind_speed(mph)|wind_gust(mph)|pressure(inHg)|precipitation(in)|         condition|           datetime|\n+-------+------------+-----------+--------------+---------------+--------------+--------------+-----------------+------------------+-------------------+\n|     46|          43|         89|            NE|             18|             0|         29.63|              0.0|               Fog|2022-12-23 00:04:00|\n|     46|          43|         89|            NE|             18|             0|         29.63|              0.0|            Cloudy|2022-12-23 00:09:00|\n|     46|          43|         89|            NE|             20|             0|         29.61|              0.0|        Light Rain|2022-12-23 00:22:00|\n|     52|          46|         82|           ESE|             16|            21|         29.59|              0.0|        Light Rain|2022-12-23 00:47:00|\n|     55|          52|         89|           ESE|             17|             0|         29.58|              0.0|        Light Rain|2022-12-23 00:51:00|\n|     56|          53|         90|           ESE|             20|            28|         29.57|              0.0|        Light Rain|2022-12-23 01:01:00|\n|     57|          53|         87|           ESE|             26|            31|         29.49|              0.0|    Cloudy / Windy|2022-12-23 01:51:00|\n|     57|          53|         87|            SE|             23|            32|         29.48|              0.0|    Cloudy / Windy|2022-12-23 01:59:00|\n|     58|          53|         84|            SE|             29|            41|         29.44|              0.0|    Cloudy / Windy|2022-12-23 02:11:00|\n|     58|          53|         84|            SE|             25|            40|         29.42|              0.0|Light Rain / Windy|2022-12-23 02:40:00|\n|     58|          53|         84|            SE|             31|            40|         29.39|              0.1|Light Rain / Windy|2022-12-23 02:51:00|\n|     58|          53|         84|            SE|             30|            41|         29.33|              0.0|   T-Storm / Windy|2022-12-23 03:27:00|\n|     58|          53|         84|            SE|             32|            49|         29.27|              0.0|   T-Storm / Windy|2022-12-23 03:51:00|\n|     58|          54|         87|             S|             20|            45|          29.3|              0.0|              Rain|2022-12-23 04:04:00|\n|     58|          54|         87|             S|             23|            32|         29.28|              0.1|Light Rain / Windy|2022-12-23 04:16:00|\n|     56|          53|         90|           SSW|              8|            23|         29.25|              0.1|        Heavy Rain|2022-12-23 04:33:00|\n|     57|          53|         87|           SSW|             16|            39|         29.26|              0.2|        Heavy Rain|2022-12-23 04:40:00|\n|     56|          53|         90|             S|             21|            32|         29.25|              0.5|Heavy Rain / Windy|2022-12-23 04:51:00|\n|     55|          54|         94|             S|             25|            32|         29.21|              0.3|Light Rain / Windy|2022-12-23 05:46:00|\n|     55|          52|         88|             S|             17|            32|         29.21|              0.3|        Light Rain|2022-12-23 05:49:00|\n+-------+------------+-----------+--------------+---------------+--------------+--------------+-----------------+------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "def combine_weather_dfs():\n",
    "    path = '/mnt/taxi_etl/weather_data/*.parquet'\n",
    "    df = spark.read.option('inferSchema','true').parquet(path)\n",
    "    df = df.drop('__index_level_0__')\n",
    "    df = df.withColumnRenamed(\"Temperature\", \"temp(f)\")\\\n",
    "       .withColumnRenamed(\"Dew_Point\", \"dew_point(f)\")\\\n",
    "       .withColumnRenamed(\"Humidity\", \"humidity(%)\")\\\n",
    "       .withColumnRenamed(\"Wind\", \"wind_direction\")\\\n",
    "       .withColumnRenamed(\"Wind_Speed\", \"wind_speed(mph)\")\\\n",
    "       .withColumnRenamed(\"Wind_Gust\", \"wind_gust(mph)\")\\\n",
    "       .withColumnRenamed(\"Pressure\", \"pressure(inHg)\")\\\n",
    "       .withColumnRenamed(\"Precipitation\", \"precipitation(in)\")\\\n",
    "       .withColumnRenamed(\"Condition\", \"condition\")\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "combine_weather_dfs()\n",
    "# Total rows = 133652 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa23d482-7de1-4e6b-9e18-c13213c6b69c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3990728261401669,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "taxi_etl",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
