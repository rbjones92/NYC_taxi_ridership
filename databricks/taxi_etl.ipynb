{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b532d9-ee21-49bf-9e61-7aa9d1368505",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# New York Taxi ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf32d9-612d-4ca6-951f-c066f5fd25a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb6a826-68fc-4bd4-b6b1-f5b422735fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43969906-da7b-4de0-bd30-d5d1d1398fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Construct Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e072d6c0-2c7f-4c6e-abfc-7e3ea7cb8802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Start SparkSession with azure hadoop package \n",
    "spark = SparkSession.builder.master('local').appName('app').config('spark.jars.packages', 'org.apache.hadoop:hadoop-azure:3.3.1').getOrCreate()        \n",
    "spark.conf.set(\"fs.azure.account.key.springboardstorage.blob.core.windows.net\",{azure_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975b4271-d0ae-4658-abe7-dde2fb5a043d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o442.mount.\n",
      ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/taxi_etl; nested exception is: \n",
      "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/taxi_etl\n",
      "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
      "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
      "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1025)\n",
      "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1051)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:555)\n",
      "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:650)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:671)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:412)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:410)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:407)\n",
      "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:455)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:440)\n",
      "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:645)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:564)\n",
      "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:555)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:525)\n",
      "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
      "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
      "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1045)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/taxi_etl\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:580)\n",
      "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:948)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:729)\n",
      "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:937)\n",
      "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:588)\n",
      "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n",
      "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
      "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
      "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$5(DbfsServerBackend.scala:387)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
      "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
      "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
      "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:387)\n",
      "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:331)\n",
      "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
      "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
      "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
      "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
      "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
      "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
      "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
      "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
      "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
      "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
      "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
      "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
      "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n",
      "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:948)\n",
      "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:540)\n",
      "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
      "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:420)\n",
      "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
      "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
      "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:55)\n",
      "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:420)\n",
      "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:179)\n",
      "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
      "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n",
      "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n",
      "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n",
      "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n",
      "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n",
      "\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n",
      "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n",
      "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n",
      "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n",
      "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n",
      "\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n",
      "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n",
      "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n",
      "\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n",
      "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:83)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
      "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
      "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:49)\n",
      "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:78)\n",
      "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n",
      "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create mount mount to connect to azure blob\n",
    "# ...Use this once or enter into try / expect block\n",
    "try:\n",
    "    dbutils.fs.mount(source = \"wasbs://springboardcontainer@springboardstorage.blob.core.windows.net\",\n",
    "    mount_point = \"/mnt/taxi_etl\",\n",
    "    extra_configs = {\"fs.azure.account.key.springboardstorage.blob.core.windows.net\": {azure_key}})\n",
    "# How to pass in java.lang.IllegalArgumentException?\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e5a310-21b6-49c1-8fd4-5303a1f2f0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [FileInfo(path='dbfs:/mnt/taxi_etl/__MACOSX/', name='__MACOSX/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/chromedriver', name='chromedriver', size=14452880, modificationTime=1677008615000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/combined_trade_and_quote/', name='combined_trade_and_quote/', size=0, modificationTime=1688771082000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/data/', name='data/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/deltalake/', name='deltalake/', size=0, modificationTime=1686166290000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/taxi_data_logs/', name='taxi_data_logs/', size=0, modificationTime=1690845652000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/test_weather_data/', name='test_weather_data/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/trip_data/', name='trip_data/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/weather_data/', name='weather_data/', size=0, modificationTime=1678395045000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/weather_data_logs/', name='weather_data_logs/', size=0, modificationTime=1690843172000)]"
     ]
    }
   ],
   "source": [
    "# View files in SpringBoard Container\n",
    "dbutils.fs.ls(\"/mnt/taxi_etl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7c2da7-c9e7-4d7f-8ffb-fec31d592f66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grab Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d5c528-19fb-4234-bd41-2a82739a7fdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Find Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976bc8a6-9c0a-4a44-a030-83c43796eaa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08']\n"
     ]
    }
   ],
   "source": [
    "# Example URLs\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def get_existing_files():\n",
    "    # Get filenames for existing files in the blob storage\n",
    "    file_list = []\n",
    "    files = dbutils.fs.ls(\"/mnt/taxi_etl/trip_data/\")\n",
    "    for x in range(len(files)):\n",
    "        file = files[x][0].split('/')[-1]\n",
    "        file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "def get_dates():\n",
    "    # Get date range for last 3 months\n",
    "    current_date = datetime.datetime.now()\n",
    "    end_date = current_date.replace(day=1).strftime('%Y-%m-%d')\n",
    "    last_month = current_date.replace(day=1)-datetime.timedelta(days=1)\n",
    "    start_date = last_month.replace(month=last_month.month-6,day=1).strftime('%Y-%m-%d')\n",
    "    yellow_dates = pd.date_range(start_date,end_date,freq='MS').strftime(\"%Y-%m\").to_list()\n",
    "    return yellow_dates\n",
    "\n",
    "def remove_existing():\n",
    "    # Compare potential new files to existing files\n",
    "    new = get_dates()\n",
    "    new_files = []\n",
    "    for x in range(len(new)):\n",
    "        new_files.append('yellow_'+new[x]+'.parquet')\n",
    "    existing = get_existing_files()\n",
    "    # Remove existing items\n",
    "    to_add = [item for item in new_files if item not in existing]\n",
    "    dates_to_add = []\n",
    "    for x in range(len(to_add)):\n",
    "        dates_to_add.append(to_add[x].split('_')[-1].split('.')[0])\n",
    "    return dates_to_add\n",
    "\n",
    "remove_existing()\n",
    "\n",
    "yellow_dates = remove_existing()\n",
    "print(yellow_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c54de9-3366-4548-af2a-8e0678145120",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions To Download and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac5c492-7b0f-4322-9b26-93a78c2bc469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ScrapeNyTaxi:\n",
    "    '''\n",
    "    Functions to loop thru select dates from remove_existing() and download each parquet file in that range for yellow taxi data\n",
    "    '''\n",
    "    def grab_yellow():\n",
    "        # For Logging\n",
    "        start_time = datetime.datetime.now()\n",
    "        start_time_str = start_time.isoformat()\n",
    "        # Download yellow cab parquet files \n",
    "        for date in yellow_dates:\n",
    "            try:\n",
    "                url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{date}.parquet'\n",
    "                response = requests.get(url, allow_redirects=True)               \n",
    "                open(f'/dbfs/mnt/taxi_etl/trip_data/yellow_{date}.parquet','wb').write(response.content)\n",
    "                # Get Time Following Write\n",
    "                end_time = datetime.datetime.now()\n",
    "                end_time_str = end_time.isoformat()\n",
    "                # Open File and Read Num Rows\n",
    "                parquet_file_path = f'/mnt/taxi_etl/trip_data/yellow_{date}.parquet'\n",
    "                # If Bad Data Pull...Remove File with No Data but keep Log\n",
    "                path = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "                onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]                \n",
    "                for file in onlyfiles:\n",
    "                    if os.path.getsize(path+file) < 250:\n",
    "                        os.remove(path+file)\n",
    "                if os.path.exists(path+f'yellow_{date}.parquet'):\n",
    "                    df = spark.read.parquet(parquet_file_path)\n",
    "                    num_rows_ingested = df.count()\n",
    "                else:\n",
    "                    num_rows_ingested = 0\n",
    "                # Logging File Names\n",
    "                file_name = f'yellow_{date}.parquet'\n",
    "                file_write_name = f'yellow_{date}.json'\n",
    "                # Log Data\n",
    "                data = {\"StartTime\":start_time_str,\"EndTime\":end_time_str,\"RowsIngested\":num_rows_ingested,\"FileName\":file_name}\n",
    "                data_json = json.dumps(data)\n",
    "                # Create the 'taxi_data_logs' directory if it doesn't exist\n",
    "                logs_dir = '/dbfs/mnt/taxi_etl/taxi_data_logs/'\n",
    "                if not os.path.exists(logs_dir):\n",
    "                    os.makedirs(logs_dir)\n",
    "                log_file_path = f'{logs_dir}{file_write_name}'\n",
    "                open(log_file_path, 'w').write(data_json)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Exception: {e}')\n",
    "                print(f'Could not write {file_name}')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576c3271-997c-480b-9bdc-5c3e970cefc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: An error occurred while calling o13933.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 188.0 failed 4 times, most recent failure: Lost task 0.3 in stage 188.0 (TID 520) (10.139.64.4 executor 0): org.apache.spark.SparkException: Exception thrown in awaitResult: Could not read footer for file: FileStatus{path=dbfs:/mnt/taxi_etl/trip_data/yellow_2023-08.parquet; isDirectory=false; length=255; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}.\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:472)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:890)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:958)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:952)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:101)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:894)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:894)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:370)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
      "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Could not read footer for file: FileStatus{path=dbfs:/mnt/taxi_etl/trip_data/yellow_2023-08.parquet; isDirectory=false; length=255; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1201)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:903)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$3(ThreadUtils.scala:549)\n",
      "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
      "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
      "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)\n",
      "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)\n",
      "\tat org.apache.spark.util.threads.CapturedSparkThreadLocals.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:549)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n",
      "\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n",
      "\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n",
      "\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n",
      "\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n",
      "Caused by: java.lang.RuntimeException: dbfs:/mnt/taxi_etl/trip_data/yellow_2023-08.parquet is not a Parquet file. Expected magic number at tail, but found [114, 111, 114, 62]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:569)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:802)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:670)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:52)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:897)\n",
      "\t... 20 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3381)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3313)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3304)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3304)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1433)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1433)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1433)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3593)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3531)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3519)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1182)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1170)\n",
      "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2750)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1070)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1068)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:95)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:963)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:59)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:96)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:232)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:225)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:457)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:378)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:334)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:334)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:762)\n",
      "\tat sun.reflect.GeneratedMethodAccessor478.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: Could not read footer for file: FileStatus{path=dbfs:/mnt/taxi_etl/trip_data/yellow_2023-08.parquet; isDirectory=false; length=255; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}.\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:472)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:890)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:958)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:952)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:101)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:894)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:894)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:370)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
      "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1713)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Could not read footer for file: FileStatus{path=dbfs:/mnt/taxi_etl/trip_data/yellow_2023-08.parquet; isDirectory=false; length=255; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1201)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:903)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$3(ThreadUtils.scala:549)\n",
      "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
      "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)\n",
      "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
      "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)\n",
      "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)\n",
      "\tat org.apache.spark.util.threads.CapturedSparkThreadLocals.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:549)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n",
      "\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n",
      "\tat java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n",
      "\tat java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n",
      "\tat java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n",
      "Caused by: java.lang.RuntimeException: dbfs:/mnt/taxi_etl/trip_data/yellow_2023-08.parquet is not a Parquet file. Expected magic number at tail, but found [114, 111, 114, 62]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:569)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:802)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:670)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:52)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:897)\n",
      "\t... 20 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ScrapeNyTaxi.grab_yellow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8adea3-8afd-49f5-aeae-ac2d32bd5b4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Remove Empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958f2fba-45ec-4bb3-8e73-9b5fdc94c430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 396 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Total number of files\n",
    "print(f'Total: {len(onlyfiles)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5c485f-e6ba-4f3c-acfd-784ca59337a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_2023-06.parquet has no data\n",
      "yellow_2023-07.parquet has no data\n",
      "yellow_2023-08.parquet has no data\n"
     ]
    }
   ],
   "source": [
    "# Several files are empty, as there was no data to pull from the web\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(path+file) < 250:\n",
    "      print(f'{file} has no data')\n",
    "      os.remove(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd3a027-3c2c-477e-8d97-728ca4b3881b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 396 files\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Total number of files\n",
    "print(f'Total: {len(onlyfiles)} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8182e5bf-9fe2-45d5-8b32-ba9783aad80b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Print Size of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc688cef-028c-4695-9201-147debf897bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow taxi data totals 18.526986873 gigs\n"
     ]
    }
   ],
   "source": [
    "yellow_bytes = 0\n",
    "for file in onlyfiles:\n",
    "    if file.startswith('yellow'):\n",
    "        yellow_bytes += os.path.getsize(path+file)\n",
    "print(f'yellow taxi data totals {yellow_bytes/1000000000} gigs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918723a9-f094-4e6c-8dc7-1b7a9848487d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explore Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032f7faa-d494-4dd9-aa3a-4b87002669bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce56e83-4086-404d-8d5a-b85e55690c27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "\n",
    "yellow_taxi = []\n",
    "for file in os.listdir(directory):\n",
    "    if file.startswith('yellow'):\n",
    "        yellow_taxi.append(file)\n",
    "\n",
    "all_data = [yellow_taxi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa207e11-fcee-48ca-916b-fc91dae1b9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema(data):\n",
    "    # Read schema for each file and write to a .json file\n",
    "    file_list = []\n",
    "    schema_list = []\n",
    "\n",
    "    for files in data:\n",
    "        df = spark.read.option('inferSchema','true').format('parquet').load(directory[5:]+files)\n",
    "        file_list.append(files)\n",
    "        schema_list.append(str(df.dtypes))\n",
    "\n",
    "    list_zip = zip(file_list,schema_list)\n",
    "    zipped_list = list(list_zip)\n",
    "\n",
    "    df_schema = StructType([ \\\n",
    "        StructField(\"File\",StringType(),True), \\\n",
    "        StructField(\"Schema\",StringType(),True),\n",
    "    ]) \n",
    "\n",
    "    df = spark.createDataFrame(zipped_list,schema= df_schema)\n",
    "    df = df.groupBy(\"Schema\").agg(F.collect_list('File'))\n",
    "    \n",
    "    data_str = data[0]\n",
    "    name = data_str.split(' ')[0]\n",
    "    \n",
    "    df.write.json(f'/dbfs/mnt/taxi_etl/trip_data/{name}_schema.json')\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca09ee5a-b3c9-4d66-863f-6d20daa44b75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: '\\nfor data in all_data:\\n    get_schema(data)\\n'"
     ]
    }
   ],
   "source": [
    "# Run schema finder for yellow data\n",
    "'''\n",
    "for data in all_data:\n",
    "    get_schema(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa0a326-98b4-4771-9b61-42d2303331ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Normalize Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59de3aa-5bb3-4e51-8b14-10ec7544b127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Define Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d864a2b-a4ad-4998-ae21-ddb5ef63aa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Schemas for each type of data \n",
    "yellow_schema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('pickup_datetime', TimestampType(), True),\n",
    "    StructField('dropoff_datetime', TimestampType(), True),\n",
    "    StructField('passenger_count', StringType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('airport_fee', IntegerType(), True),\n",
    "    StructField('taxi_type', StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77afc5a6-ecc6-4573-bc32-68bf86b7eb39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Cast Each Group's Schema and Union to Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e002410-1a65-4296-b821-b66bdf3d8bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Script to make yellow schema\n",
    "def make_yellow():\n",
    "\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    yellow_df = spark.createDataFrame(emptyRDD,schema=yellow_schema)\n",
    "\n",
    "    yellow_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('yellow'):\n",
    "            yellow_list.append(file)    \n",
    "\n",
    "    for file in yellow_list:\n",
    "        df_yellow = spark.read.option('inferSchema','true').parquet(f'{directory[5:]}{file}')\n",
    "        df_yellow = df_yellow.withColumn('taxi_type',lit('yellow'))\n",
    "        df_yellow = df_yellow.withColumnRenamed('tpep_pickup_datetime','pickup_datetime')\\\n",
    "            .withColumnRenamed('tpep_dropoff_datetime','dropoff_datetime')\n",
    "\n",
    "        df_yellow.createOrReplaceTempView('Cast')\n",
    "\n",
    "        df_yellow = spark.sql(\"SELECT BIGINT(VendorID),TIMESTAMP(pickup_datetime),\\\n",
    "            TIMESTAMP(dropoff_datetime),DOUBLE(passenger_count),DOUBLE(trip_distance),\\\n",
    "            BIGINT(RatecodeID),STRING(store_and_fwd_flag),BIGINT(PULocationID),BIGINT(DOLocationID),\\\n",
    "            BIGINT(payment_type),DOUBLE(fare_amount),DOUBLE(extra),DOUBLE(mta_tax),DOUBLE(tip_amount),\\\n",
    "            DOUBLE(tolls_amount),DOUBLE(improvement_surcharge),DOUBLE(total_amount),DOUBLE(congestion_surcharge),\\\n",
    "            DOUBLE(airport_fee),STRING(taxi_type) from Cast\")\n",
    "\n",
    "        yellow_df = df_yellow.union(yellow_df)\n",
    "        print(f'{file} analyzed')\n",
    "\n",
    "    yellow_df.printSchema()\n",
    "\n",
    "    return yellow_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76da95d-1a0d-4b09-ad24-e935f4a39356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test read of dataframe\n",
    "def read_data():\n",
    "    # Test read all yellow data\n",
    "    yellow_df = spark.read.format('parquet').load(directory[5:]+'yellow*.parquet')\n",
    "    yellow_df.printSchema()\n",
    "read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3e5704-56ed-437c-ac90-c59de2a925b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grab New York Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81f577f-c5bb-479c-a416-ebcd5596496a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edfe3bf-be7f-46e6-80bf-a62b70df1f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: selenium in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (4.11.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /databricks/python3/lib/python3.9/site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /databricks/python3/lib/python3.9/site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: idna in /databricks/python3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /databricks/python3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.1.2)\n",
      "Requirement already satisfied: outcome in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c55c894-20a3-4b30-a205-8e95e8cbe756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46224362-c2c1-4237-b189-dd614a540525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Install Chrome and ChromeDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c84c38-553a-44d0-b539-90cf7da501e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1045 bytes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks/scripts/selenium-install.sh</td><td>selenium-install.sh</td><td>1045</td><td>1690915264000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks/scripts/selenium-install.sh",
         "selenium-install.sh",
         1045,
         1690915264000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "dbutils.fs.put(\"/databricks/scripts/selenium-install.sh\",\"\"\"\n",
    "#!/bin/bash\n",
    "%sh\n",
    "LAST_VERSION=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2FLAST_CHANGE?alt=media\"\n",
    "VERSION=$(curl -s -S $LAST_VERSION)\n",
    "if [ -d $VERSION ] ; then\n",
    "  echo \"version already installed\"\n",
    "  exit\n",
    "fi\n",
    " \n",
    "rm -rf /tmp/chrome/$VERSION\n",
    "mkdir -p /tmp/chrome/$VERSION\n",
    " \n",
    "URL=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F$VERSION%2Fchrome-linux.zip?alt=media\"\n",
    "ZIP=\"${VERSION}-chrome-linux.zip\"\n",
    " \n",
    "curl -# $URL > /tmp/chrome/$ZIP\n",
    "unzip /tmp/chrome/$ZIP -d /tmp/chrome/$VERSION\n",
    " \n",
    "URL=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F$VERSION%2Fchromedriver_linux64.zip?alt=media\"\n",
    "ZIP=\"${VERSION}-chromedriver_linux64.zip\"\n",
    " \n",
    "curl -# $URL > /tmp/chrome/$ZIP\n",
    "unzip /tmp/chrome/$ZIP -d /tmp/chrome/$VERSION\n",
    " \n",
    "mkdir -p /tmp/chrome/chrome-user-data-dir\n",
    " \n",
    "rm -f /tmp/chrome/latest\n",
    "ln -s /tmp/chrome/$VERSION /tmp/chrome/latest\n",
    " \n",
    "# to avoid errors about missing libraries\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y libgbm-dev\n",
    "\"\"\", True)\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b28efa-9b8d-4570-bba4-21db81d677b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dbfs/databricks/scripts/selenium-install.sh: line 3: fg: no job control\n",
      "\n",
      "                                                                           0.0%\n",
      "#                                                                          2.5%\n",
      "######                                                                     8.8%\n",
      "###########                                                               15.5%\n",
      "###############                                                           22.1%\n",
      "####################                                                      28.8%\n",
      "#########################                                                 35.6%\n",
      "##############################                                            42.5%\n",
      "###################################                                       49.5%\n",
      "########################################                                  56.6%\n",
      "############################################                              62.2%\n",
      "################################################                          67.6%\n",
      "#####################################################                     74.1%\n",
      "##########################################################                80.6%\n",
      "##############################################################            87.1%\n",
      "###################################################################       93.4%\n",
      "######################################################################## 100.0%\n",
      "Archive:  /tmp/chrome/1177895-chrome-linux.zip\n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/MEIPreload/manifest.json  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/MEIPreload/preloaded_data.pb  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/chrome  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/chrome-wrapper  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/chrome_100_percent.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/chrome_200_percent.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/chrome_crashpad_handler  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/chrome_sandbox  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/icudtl.dat  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/libEGL.so  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/libGLESv2.so  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/libvk_swiftshader.so  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/libvulkan.so.1  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/nacl_helper  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/nacl_helper_bootstrap  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/nacl_irt_x86_64.nexe  \n",
      " extracting: /tmp/chrome/1177895/chrome-linux/product_logo_48.png  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/resources.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/v8_context_snapshot.bin  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/vk_swiftshader_icd.json  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/xdg-mime  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/xdg-settings  \n",
      "   creating: /tmp/chrome/1177895/chrome-linux/ClearKeyCdm/\n",
      "   creating: /tmp/chrome/1177895/chrome-linux/ClearKeyCdm/_platform_specific/\n",
      "   creating: /tmp/chrome/1177895/chrome-linux/ClearKeyCdm/_platform_specific/linux_x64/\n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/ClearKeyCdm/_platform_specific/linux_x64/libclearkeycdm.so  \n",
      "   creating: /tmp/chrome/1177895/chrome-linux/locales/\n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/pt-BR.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ur.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fil.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ur.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/mr.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/hi.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/am.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/it.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ru.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/en-GB.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/he.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/gu.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/en-XA.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/zh-CN.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/zh-TW.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ar-XB.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/da.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/bg.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/en-XA.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ta.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/id.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/kn.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ca.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/id.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ro.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fi.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/bg.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fr.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ar-XB.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/cs.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sl.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/pl.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/et.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/zh-TW.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/vi.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/lt.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/es-419.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/mr.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fa.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/pt-BR.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/lv.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ar.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ta.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ja.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/nb.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/th.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ms.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ms.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/el.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fa.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ro.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/tr.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ko.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ml.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sr.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/kn.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/nb.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/hu.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ja.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/da.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/pt-PT.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/es-419.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sl.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/af.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/nl.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/it.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/tr.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sv.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/hr.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fi.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/cs.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/uk.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/et.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/th.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ar.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/zh-CN.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ko.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/hu.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/am.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/en-GB.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/uk.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/gu.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sw.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/af.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/pl.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/pt-PT.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fil.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/bn.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/te.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/te.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/es.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/hi.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/vi.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/fr.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sr.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/bn.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ml.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/en-US.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/de.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sw.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/he.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ca.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/de.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sk.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/es.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/ru.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/lv.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sv.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/lt.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/el.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/en-US.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/nl.pak  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/sk.pak.info  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/locales/hr.pak  \n",
      "   creating: /tmp/chrome/1177895/chrome-linux/resources/\n",
      "   creating: /tmp/chrome/1177895/chrome-linux/resources/inspector_overlay/\n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/resources/inspector_overlay/main.js  \n",
      "  inflating: /tmp/chrome/1177895/chrome-linux/resources/inspector_overlay/inspector_overlay_resources.grd  \n",
      "#=#=#                                                                         \n",
      "\n",
      "#####                                                                      7.4%\n",
      "####################################                                      51.2%\n",
      "######################################################################## 100.0%\n",
      "Archive:  /tmp/chrome/1177895-chromedriver_linux64.zip\n",
      "  inflating: /tmp/chrome/1177895/chromedriver_linux64/LICENSE.chromedriver  \n",
      "  inflating: /tmp/chrome/1177895/chromedriver_linux64/chromedriver  \n",
      "Hit:1 https://repos.azul.com/zulu/deb stable InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "libgbm-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "/dbfs/databricks/scripts/selenium-install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63f0686-fbfc-4031-9fb3-2b70aec10f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "s = Service('/tmp/chrome/latest/chromedriver_linux64/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"/tmp/chrome/latest/chrome-linux/chrome\"\n",
    "options.add_argument('headless')\n",
    "options.add_argument('--disable-infobars')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--remote-debugging-port=9222')\n",
    "options.add_argument('--homedir=/tmp/chrome/chrome-user-data-dir')\n",
    "options.add_argument('--user-data-dir=/tmp/chrome/chrome-user-data-dir')\n",
    "prefs = {\"download.default_directory\":\"/tmp/chrome/chrome-user-data-di\",\n",
    "         \"download.prompt_for_download\":False\n",
    "}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "# driver = webdriver.Chrome(service=s, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37c473-b382-4197-877f-8a5eb3abfd83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Load Wunderground Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bb2776-8b28-4dfa-b0ec-1950bd1f5de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: True"
     ]
    }
   ],
   "source": [
    "# Create Log Directory\n",
    "dbutils.fs.mkdirs(\"/mnt/taxi_etl/weather_data_logs/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9563dc6-21d0-4ba3-bcc7-cf040c8112d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Last Three Months of Weather Data\n",
    "def get_existing_files():\n",
    "    # Get filenames for existing files in the blob storage\n",
    "    file_list = []\n",
    "    files = dbutils.fs.ls(\"/mnt/taxi_etl/weather_data/\")\n",
    "    for x in range(len(files)):\n",
    "        file = files[x][0].split('/')[-1][10:20]\n",
    "        file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "def last_three_months():\n",
    "    from datetime import date, timedelta\n",
    "    # Get dates for last three months\n",
    "    current_date = date.today()\n",
    "    three_months_ago = current_date - timedelta(days=6\n",
    "                                                *30)  # Approximating 30 days per month\n",
    "    date_list = []\n",
    "    while current_date >= three_months_ago:\n",
    "        date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date -= timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "def get_dates():\n",
    "    existing = get_existing_files()\n",
    "    new_dates = last_three_months()\n",
    "    to_add = [item for item in new_dates if item not in existing]\n",
    "    return to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc50a38-70cd-49c3-aee0-9dd216ee6b18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to load wunderground data (without this it has no records to show)\n",
    "def render_page(url):\n",
    "    driver = webdriver.Chrome(service=s, options=options)    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    r = driver.page_source\n",
    "    driver.quit()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af48ef43-0154-48af-9b50-93efc0a230d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_transpose(data_list):\n",
    "    res_list = [[item.replace('%', '') for item in lst] for lst in data_list]\n",
    "    res_list = [[item.replace(u'\\xa0', u'') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('F','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('in','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('%','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('mph','') for item in lst] for lst in res_list]\n",
    "    final_list = [[item.replace('','') for item in lst] for lst in res_list]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94df9798-cad6-428a-8316-b8314725df8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def set_schema(df):\n",
    "    # To Interger\n",
    "    df[[\"Temperature\",\"Dew_Point\", \"Humidity\",\"Wind_Speed\",\"Wind_Gust\"]] = df[[\"Temperature\",\"Dew_Point\", \"Humidity\",\"Wind_Speed\",\"Wind_Gust\"]].apply(pd.to_numeric)\n",
    "    df[['Pressure','Precipitation']] = df[['Pressure','Precipitation']].apply(pd.to_numeric)\n",
    "    # To DateTime\n",
    "    df['datetime'] = df['datetime'].apply(pd.to_datetime)\n",
    "    # To String\n",
    "    df[['Wind','Condition']] = df[['Wind','Condition']].applymap(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958101fd-76d4-4ad9-b70e-227e079bc881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-27\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-26\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-25\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-24\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-23\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-22\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-21\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-20\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-19\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-18\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-17\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-16\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-15\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-14\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-13\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-12\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-11\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-10\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-09\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-08\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-07\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-06\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-05\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-04\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-03\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-02\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def scraper(page, dates):\n",
    "    '''function to scrape wunderground'''\n",
    "    for d in dates:\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        url = str(str(page) + str(d))\n",
    "        print(url)\n",
    "        r = render_page(url)\n",
    "\n",
    "        soup = BS(r, \"html.parser\")\n",
    "        container = soup.find('lib-city-history-observation')\n",
    "        check = container.find('tbody')\n",
    "\n",
    "        data = []\n",
    "        try:\n",
    "            for c in check.find_all('tr', class_='ng-star-inserted'):\n",
    "                for i in c.find_all('td', class_='ng-star-inserted'):\n",
    "                    trial = i.text\n",
    "                    trial = trial.strip('  ')\n",
    "                    data.append(trial)\n",
    "            \n",
    "            df_daily = []\n",
    "            cols = ['Time','Temperature','Dew_Point','Humidity','Wind','Wind_Speed','Wind_Gust','Pressure','Precipitation','Condition','Date']\n",
    "            for i in range(0,len(data),10):\n",
    "                snip_data = []\n",
    "                snip_data.append(data[i:i+10])\n",
    "                # Strip of Weird Characters\n",
    "                snip_data = list_transpose(snip_data)\n",
    "                snip_data[0].append(d)\n",
    "                df = pd.DataFrame(snip_data,columns=cols)\n",
    "                df['datetime'] = df['Date'] + ' ' + df['Time']\n",
    "                df = df.drop(['Date','Time'],axis=1) \n",
    "                # Set Schema\n",
    "                df = set_schema(df)\n",
    "                df_daily.append(df)\n",
    "\n",
    "            df_daily = pd.concat(df_daily)\n",
    "            num_rows_ingested = len(df_daily.index)\n",
    "            df_daily.to_parquet(f'/dbfs/mnt/taxi_etl/weather_data/NY_Weather{d}.parquet')\n",
    "\n",
    "            end_time = datetime.now()\n",
    "\n",
    "            # Logging JSON\n",
    "            file_name = f'NY_Weather{d}.parquet'\n",
    "            file_write_name = f'NY_Weather{d}.json'\n",
    "            start_time_str = start_time.isoformat()\n",
    "            end_time_str = end_time.isoformat()\n",
    "            data = {\"StartTime\":start_time_str,\"EndTime\":end_time_str,\"RowsIngested\":num_rows_ingested,\"FileName\":file_name}\n",
    "            data_json = json.dumps(data)\n",
    "            open(f'/dbfs/mnt/taxi_etl/weather_data_logs/{file_write_name}','w').write(data_json)\n",
    "            \n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "# Call Functions\n",
    "dates = get_dates()\n",
    "page = 'https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/'\n",
    "\n",
    "scraper(page, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819824f7-e492-40e7-aabe-c9657d5da0ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh du -h /dbfs/mnt/taxi_etl/weather_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e135e588-4928-4d1c-b27d-43fb0d26cc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh ls /dbfs/mnt/taxi_etl/weather_data/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed63171e-c590-48b9-9c50-f73f190b07bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make Directory\n",
    "dbutils.fs.mkdirs(\"/mnt/taxi_etl/weather_data/combined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6933f502-e8cb-4e7f-8398-4269b8d1cd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine weather data to check that schema is readable for all files\n",
    "def combine_weather_dfs():\n",
    "    path = '/mnt/taxi_etl/weather_data/*.parquet'\n",
    "    df = spark.read.option('inferSchema','true').parquet(path)\n",
    "    df = df.drop('__index_level_0__')\n",
    "    df = df.withColumnRenamed(\"Temperature\", \"temp(f)\")\\\n",
    "       .withColumnRenamed(\"Dew_Point\", \"dew_point(f)\")\\\n",
    "       .withColumnRenamed(\"Humidity\", \"humidity(%)\")\\\n",
    "       .withColumnRenamed(\"Wind\", \"wind_direction\")\\\n",
    "       .withColumnRenamed(\"Wind_Speed\", \"wind_speed(mph)\")\\\n",
    "       .withColumnRenamed(\"Wind_Gust\", \"wind_gust(mph)\")\\\n",
    "       .withColumnRenamed(\"Pressure\", \"pressure(inHg)\")\\\n",
    "       .withColumnRenamed(\"Precipitation\", \"precipitation(in)\")\\\n",
    "       .withColumnRenamed(\"Condition\", \"condition\")\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "combine_weather_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa23d482-7de1-4e6b-9e18-c13213c6b69c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### DeltaLake Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "effc35e0-e4d5-4830-9027-39a078ea99de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Convert parquet to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e710ba-3d53-49e4-975b-783e6bd0747e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read All Parquet Files from deltalake/parquet/\n",
    "path = '/mnt/taxi_etl/deltalake/parquet/*.parquet'\n",
    "df = spark.read.option('inferSchema','true').parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425b4d0a-8a60-4505-875b-3684bf95ed83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592a796a-389e-4c5f-af13-5027883b02f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save dataframe to delta\n",
    "delta_path = '/mnt/taxi_etl/deltalake/delta'\n",
    "df.coalesce(1).write.format('delta').mode('overwrite').save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f605eb-cf9e-4ea7-b772-53be545b6e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# See files\n",
    "dbutils.fs.ls(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0a662a-924c-4dec-8234-fa5398f7fc6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View each log individually\n",
    "log_directory = '/mnt/taxi_etl/deltalake/delta/_delta_log/'\n",
    "files = dbutils.fs.ls(log_directory)\n",
    "filenames = [file.name for file in files if file.name.endswith('.json')]\n",
    "for filename in filenames:\n",
    "    display(spark.read.text(log_directory+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "849a319f-0f43-4d63-bff8-a32661212d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Analyze Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba8cc81-0b09-4d13-9336-00d525ca4253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Key metrics can be visualized and data can be examined in the Data Profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0698b1d-6ed5-4fb3-af95-35d1a38b6d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM delta.`/mnt/taxi_etl/deltalake/delta/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f03df201-4a6c-4d39-9e93-c91d279e3572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# More summary statistics can be found by using describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4aee42-1957-494c-8141-f1c0f791b84e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/mnt/taxi_etl/deltalake/delta/'\n",
    "df = spark.read.format(\"delta\").load(file_path)\n",
    "df.describe().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 465028284393408,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "taxi_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
