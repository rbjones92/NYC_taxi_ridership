{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b532d9-ee21-49bf-9e61-7aa9d1368505",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# New York Taxi ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf32d9-612d-4ca6-951f-c066f5fd25a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb6a826-68fc-4bd4-b6b1-f5b422735fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43969906-da7b-4de0-bd30-d5d1d1398fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Construct Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e072d6c0-2c7f-4c6e-abfc-7e3ea7cb8802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Start SparkSession with azure hadoop package \n",
    "spark = SparkSession.builder.master('local').appName('app').config('spark.jars.packages', 'org.apache.hadoop:hadoop-azure:3.3.1').getOrCreate()        \n",
    "spark.conf.set(\"fs.azure.account.key.springboardstorage.blob.core.windows.net\",{azure_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975b4271-d0ae-4658-abe7-dde2fb5a043d",
     "showTitle": false,
     "title": ""
    }
   },
   ],
   "source": [
    "# Create mount mount to connect to azure blob\n",
    "# ...Use this once or enter into try / expect block\n",
    "try:\n",
    "    dbutils.fs.mount(source = \"wasbs://springboardcontainer@springboardstorage.blob.core.windows.net\",\n",
    "    mount_point = \"/mnt/taxi_etl\",\n",
    "    extra_configs = {\"fs.azure.account.key.springboardstorage.blob.core.windows.net\": {azure_key}})\n",
    "# How to pass in java.lang.IllegalArgumentException?\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e5a310-21b6-49c1-8fd4-5303a1f2f0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [FileInfo(path='dbfs:/mnt/taxi_etl/__MACOSX/', name='__MACOSX/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/chromedriver', name='chromedriver', size=14452880, modificationTime=1677008615000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/combined_trade_and_quote/', name='combined_trade_and_quote/', size=0, modificationTime=1688771082000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/data/', name='data/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/deltalake/', name='deltalake/', size=0, modificationTime=1686166290000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/taxi_data_logs/', name='taxi_data_logs/', size=0, modificationTime=1690845652000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/test_weather_data/', name='test_weather_data/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/trip_data/', name='trip_data/', size=0, modificationTime=0),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/weather_data/', name='weather_data/', size=0, modificationTime=1678395045000),\n",
      " FileInfo(path='dbfs:/mnt/taxi_etl/weather_data_logs/', name='weather_data_logs/', size=0, modificationTime=1690843172000)]"
     ]
    }
   ],
   "source": [
    "# View files in SpringBoard Container\n",
    "dbutils.fs.ls(\"/mnt/taxi_etl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7c2da7-c9e7-4d7f-8ffb-fec31d592f66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grab Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d5c528-19fb-4234-bd41-2a82739a7fdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Find Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976bc8a6-9c0a-4a44-a030-83c43796eaa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08']\n"
     ]
    }
   ],
   "source": [
    "# Example URLs\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2022-01.parquet\n",
    "# https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def get_existing_files():\n",
    "    # Get filenames for existing files in the blob storage\n",
    "    file_list = []\n",
    "    files = dbutils.fs.ls(\"/mnt/taxi_etl/trip_data/\")\n",
    "    for x in range(len(files)):\n",
    "        file = files[x][0].split('/')[-1]\n",
    "        file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "def get_dates():\n",
    "    # Get date range for last 3 months\n",
    "    current_date = datetime.datetime.now()\n",
    "    end_date = current_date.replace(day=1).strftime('%Y-%m-%d')\n",
    "    last_month = current_date.replace(day=1)-datetime.timedelta(days=1)\n",
    "    start_date = last_month.replace(month=last_month.month-6,day=1).strftime('%Y-%m-%d')\n",
    "    yellow_dates = pd.date_range(start_date,end_date,freq='MS').strftime(\"%Y-%m\").to_list()\n",
    "    return yellow_dates\n",
    "\n",
    "def remove_existing():\n",
    "    # Compare potential new files to existing files\n",
    "    new = get_dates()\n",
    "    new_files = []\n",
    "    for x in range(len(new)):\n",
    "        new_files.append('yellow_'+new[x]+'.parquet')\n",
    "    existing = get_existing_files()\n",
    "    # Remove existing items\n",
    "    to_add = [item for item in new_files if item not in existing]\n",
    "    dates_to_add = []\n",
    "    for x in range(len(to_add)):\n",
    "        dates_to_add.append(to_add[x].split('_')[-1].split('.')[0])\n",
    "    return dates_to_add\n",
    "\n",
    "remove_existing()\n",
    "\n",
    "yellow_dates = remove_existing()\n",
    "print(yellow_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c54de9-3366-4548-af2a-8e0678145120",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions To Download and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac5c492-7b0f-4322-9b26-93a78c2bc469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ScrapeNyTaxi:\n",
    "    '''\n",
    "    Functions to loop thru select dates from remove_existing() and download each parquet file in that range for yellow taxi data\n",
    "    '''\n",
    "    def grab_yellow():\n",
    "        # For Logging\n",
    "        start_time = datetime.datetime.now()\n",
    "        start_time_str = start_time.isoformat()\n",
    "        # Download yellow cab parquet files \n",
    "        for date in yellow_dates:\n",
    "            try:\n",
    "                url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{date}.parquet'\n",
    "                response = requests.get(url, allow_redirects=True)               \n",
    "                open(f'/dbfs/mnt/taxi_etl/trip_data/yellow_{date}.parquet','wb').write(response.content)\n",
    "                # Get Time Following Write\n",
    "                end_time = datetime.datetime.now()\n",
    "                end_time_str = end_time.isoformat()\n",
    "                # Open File and Read Num Rows\n",
    "                parquet_file_path = f'/mnt/taxi_etl/trip_data/yellow_{date}.parquet'\n",
    "                # If Bad Data Pull...Remove File with No Data but keep Log\n",
    "                path = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "                onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]                \n",
    "                for file in onlyfiles:\n",
    "                    if os.path.getsize(path+file) < 250:\n",
    "                        os.remove(path+file)\n",
    "                if os.path.exists(path+f'yellow_{date}.parquet'):\n",
    "                    df = spark.read.parquet(parquet_file_path)\n",
    "                    num_rows_ingested = df.count()\n",
    "                else:\n",
    "                    num_rows_ingested = 0\n",
    "                # Logging File Names\n",
    "                file_name = f'yellow_{date}.parquet'\n",
    "                file_write_name = f'yellow_{date}.json'\n",
    "                # Log Data\n",
    "                data = {\"StartTime\":start_time_str,\"EndTime\":end_time_str,\"RowsIngested\":num_rows_ingested,\"FileName\":file_name}\n",
    "                data_json = json.dumps(data)\n",
    "                # Create the 'taxi_data_logs' directory if it doesn't exist\n",
    "                logs_dir = '/dbfs/mnt/taxi_etl/taxi_data_logs/'\n",
    "                if not os.path.exists(logs_dir):\n",
    "                    os.makedirs(logs_dir)\n",
    "                log_file_path = f'{logs_dir}{file_write_name}'\n",
    "                open(log_file_path, 'w').write(data_json)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Exception: {e}')\n",
    "                print(f'Could not write {file_name}')\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576c3271-997c-480b-9bdc-5c3e970cefc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": 
    }
   ],
   "source": [
    "ScrapeNyTaxi.grab_yellow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8adea3-8afd-49f5-aeae-ac2d32bd5b4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Remove Empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958f2fba-45ec-4bb3-8e73-9b5fdc94c430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 396 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Total number of files\n",
    "print(f'Total: {len(onlyfiles)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5c485f-e6ba-4f3c-acfd-784ca59337a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_2023-06.parquet has no data\n",
      "yellow_2023-07.parquet has no data\n",
      "yellow_2023-08.parquet has no data\n"
     ]
    }
   ],
   "source": [
    "# Several files are empty, as there was no data to pull from the web\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(path+file) < 250:\n",
    "      print(f'{file} has no data')\n",
    "      os.remove(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd3a027-3c2c-477e-8d97-728ca4b3881b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 396 files\n"
     ]
    }
   ],
   "source": [
    "onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Total number of files\n",
    "print(f'Total: {len(onlyfiles)} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8182e5bf-9fe2-45d5-8b32-ba9783aad80b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Print Size of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc688cef-028c-4695-9201-147debf897bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow taxi data totals 18.526986873 gigs\n"
     ]
    }
   ],
   "source": [
    "yellow_bytes = 0\n",
    "for file in onlyfiles:\n",
    "    if file.startswith('yellow'):\n",
    "        yellow_bytes += os.path.getsize(path+file)\n",
    "print(f'yellow taxi data totals {yellow_bytes/1000000000} gigs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918723a9-f094-4e6c-8dc7-1b7a9848487d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Explore Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032f7faa-d494-4dd9-aa3a-4b87002669bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce56e83-4086-404d-8d5a-b85e55690c27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory = '/dbfs/mnt/taxi_etl/trip_data/'\n",
    "\n",
    "yellow_taxi = []\n",
    "for file in os.listdir(directory):\n",
    "    if file.startswith('yellow'):\n",
    "        yellow_taxi.append(file)\n",
    "\n",
    "all_data = [yellow_taxi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa207e11-fcee-48ca-916b-fc91dae1b9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema(data):\n",
    "    # Read schema for each file and write to a .json file\n",
    "    file_list = []\n",
    "    schema_list = []\n",
    "\n",
    "    for files in data:\n",
    "        df = spark.read.option('inferSchema','true').format('parquet').load(directory[5:]+files)\n",
    "        file_list.append(files)\n",
    "        schema_list.append(str(df.dtypes))\n",
    "\n",
    "    list_zip = zip(file_list,schema_list)\n",
    "    zipped_list = list(list_zip)\n",
    "\n",
    "    df_schema = StructType([ \\\n",
    "        StructField(\"File\",StringType(),True), \\\n",
    "        StructField(\"Schema\",StringType(),True),\n",
    "    ]) \n",
    "\n",
    "    df = spark.createDataFrame(zipped_list,schema= df_schema)\n",
    "    df = df.groupBy(\"Schema\").agg(F.collect_list('File'))\n",
    "    \n",
    "    data_str = data[0]\n",
    "    name = data_str.split(' ')[0]\n",
    "    \n",
    "    df.write.json(f'/dbfs/mnt/taxi_etl/trip_data/{name}_schema.json')\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca09ee5a-b3c9-4d66-863f-6d20daa44b75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: '\\nfor data in all_data:\\n    get_schema(data)\\n'"
     ]
    }
   ],
   "source": [
    "# Run schema finder for yellow data\n",
    "'''\n",
    "for data in all_data:\n",
    "    get_schema(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa0a326-98b4-4771-9b61-42d2303331ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Normalize Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59de3aa-5bb3-4e51-8b14-10ec7544b127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Define Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d864a2b-a4ad-4998-ae21-ddb5ef63aa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Schemas for each type of data \n",
    "yellow_schema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('pickup_datetime', TimestampType(), True),\n",
    "    StructField('dropoff_datetime', TimestampType(), True),\n",
    "    StructField('passenger_count', StringType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('airport_fee', IntegerType(), True),\n",
    "    StructField('taxi_type', StringType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77afc5a6-ecc6-4573-bc32-68bf86b7eb39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Cast Each Group's Schema and Union to Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e002410-1a65-4296-b821-b66bdf3d8bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Script to make yellow schema\n",
    "def make_yellow():\n",
    "\n",
    "    emptyRDD = spark.sparkContext.emptyRDD()\n",
    "    yellow_df = spark.createDataFrame(emptyRDD,schema=yellow_schema)\n",
    "\n",
    "    yellow_list = []\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('yellow'):\n",
    "            yellow_list.append(file)    \n",
    "\n",
    "    for file in yellow_list:\n",
    "        df_yellow = spark.read.option('inferSchema','true').parquet(f'{directory[5:]}{file}')\n",
    "        df_yellow = df_yellow.withColumn('taxi_type',lit('yellow'))\n",
    "        df_yellow = df_yellow.withColumnRenamed('tpep_pickup_datetime','pickup_datetime')\\\n",
    "            .withColumnRenamed('tpep_dropoff_datetime','dropoff_datetime')\n",
    "\n",
    "        df_yellow.createOrReplaceTempView('Cast')\n",
    "\n",
    "        df_yellow = spark.sql(\"SELECT BIGINT(VendorID),TIMESTAMP(pickup_datetime),\\\n",
    "            TIMESTAMP(dropoff_datetime),DOUBLE(passenger_count),DOUBLE(trip_distance),\\\n",
    "            BIGINT(RatecodeID),STRING(store_and_fwd_flag),BIGINT(PULocationID),BIGINT(DOLocationID),\\\n",
    "            BIGINT(payment_type),DOUBLE(fare_amount),DOUBLE(extra),DOUBLE(mta_tax),DOUBLE(tip_amount),\\\n",
    "            DOUBLE(tolls_amount),DOUBLE(improvement_surcharge),DOUBLE(total_amount),DOUBLE(congestion_surcharge),\\\n",
    "            DOUBLE(airport_fee),STRING(taxi_type) from Cast\")\n",
    "\n",
    "        yellow_df = df_yellow.union(yellow_df)\n",
    "        print(f'{file} analyzed')\n",
    "\n",
    "    yellow_df.printSchema()\n",
    "\n",
    "    return yellow_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a76da95d-1a0d-4b09-ad24-e935f4a39356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test read of dataframe\n",
    "def read_data():\n",
    "    # Test read all yellow data\n",
    "    yellow_df = spark.read.format('parquet').load(directory[5:]+'yellow*.parquet')\n",
    "    yellow_df.printSchema()\n",
    "read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3e5704-56ed-437c-ac90-c59de2a925b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grab New York Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81f577f-c5bb-479c-a416-ebcd5596496a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edfe3bf-be7f-46e6-80bf-a62b70df1f84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: selenium in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (4.11.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /databricks/python3/lib/python3.9/site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /databricks/python3/lib/python3.9/site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: idna in /databricks/python3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /databricks/python3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.1.2)\n",
      "Requirement already satisfied: outcome in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9b762636-51c5-4d09-a377-91eaaaf30761/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c55c894-20a3-4b30-a205-8e95e8cbe756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46224362-c2c1-4237-b189-dd614a540525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Install Chrome and ChromeDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c84c38-553a-44d0-b539-90cf7da501e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1045 bytes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks/scripts/selenium-install.sh</td><td>selenium-install.sh</td><td>1045</td><td>1690915264000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks/scripts/selenium-install.sh",
         "selenium-install.sh",
         1045,
         1690915264000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "dbutils.fs.put(\"/databricks/scripts/selenium-install.sh\",\"\"\"\n",
    "#!/bin/bash\n",
    "%sh\n",
    "LAST_VERSION=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2FLAST_CHANGE?alt=media\"\n",
    "VERSION=$(curl -s -S $LAST_VERSION)\n",
    "if [ -d $VERSION ] ; then\n",
    "  echo \"version already installed\"\n",
    "  exit\n",
    "fi\n",
    " \n",
    "rm -rf /tmp/chrome/$VERSION\n",
    "mkdir -p /tmp/chrome/$VERSION\n",
    " \n",
    "URL=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F$VERSION%2Fchrome-linux.zip?alt=media\"\n",
    "ZIP=\"${VERSION}-chrome-linux.zip\"\n",
    " \n",
    "curl -# $URL > /tmp/chrome/$ZIP\n",
    "unzip /tmp/chrome/$ZIP -d /tmp/chrome/$VERSION\n",
    " \n",
    "URL=\"https://www.googleapis.com/download/storage/v1/b/chromium-browser-snapshots/o/Linux_x64%2F$VERSION%2Fchromedriver_linux64.zip?alt=media\"\n",
    "ZIP=\"${VERSION}-chromedriver_linux64.zip\"\n",
    " \n",
    "curl -# $URL > /tmp/chrome/$ZIP\n",
    "unzip /tmp/chrome/$ZIP -d /tmp/chrome/$VERSION\n",
    " \n",
    "mkdir -p /tmp/chrome/chrome-user-data-dir\n",
    " \n",
    "rm -f /tmp/chrome/latest\n",
    "ln -s /tmp/chrome/$VERSION /tmp/chrome/latest\n",
    " \n",
    "# to avoid errors about missing libraries\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y libgbm-dev\n",
    "\"\"\", True)\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b28efa-9b8d-4570-bba4-21db81d677b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "%sh\n",
    "/dbfs/databricks/scripts/selenium-install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63f0686-fbfc-4031-9fb3-2b70aec10f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "s = Service('/tmp/chrome/latest/chromedriver_linux64/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"/tmp/chrome/latest/chrome-linux/chrome\"\n",
    "options.add_argument('headless')\n",
    "options.add_argument('--disable-infobars')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--remote-debugging-port=9222')\n",
    "options.add_argument('--homedir=/tmp/chrome/chrome-user-data-dir')\n",
    "options.add_argument('--user-data-dir=/tmp/chrome/chrome-user-data-dir')\n",
    "prefs = {\"download.default_directory\":\"/tmp/chrome/chrome-user-data-di\",\n",
    "         \"download.prompt_for_download\":False\n",
    "}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "# driver = webdriver.Chrome(service=s, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd37c473-b382-4197-877f-8a5eb3abfd83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Load Wunderground Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bb2776-8b28-4dfa-b0ec-1950bd1f5de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: True"
     ]
    }
   ],
   "source": [
    "# Create Log Directory\n",
    "dbutils.fs.mkdirs(\"/mnt/taxi_etl/weather_data_logs/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9563dc6-21d0-4ba3-bcc7-cf040c8112d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Last Three Months of Weather Data\n",
    "def get_existing_files():\n",
    "    # Get filenames for existing files in the blob storage\n",
    "    file_list = []\n",
    "    files = dbutils.fs.ls(\"/mnt/taxi_etl/weather_data/\")\n",
    "    for x in range(len(files)):\n",
    "        file = files[x][0].split('/')[-1][10:20]\n",
    "        file_list.append(file)\n",
    "    return file_list\n",
    "\n",
    "def last_three_months():\n",
    "    from datetime import date, timedelta\n",
    "    # Get dates for last three months\n",
    "    current_date = date.today()\n",
    "    three_months_ago = current_date - timedelta(days=6\n",
    "                                                *30)  # Approximating 30 days per month\n",
    "    date_list = []\n",
    "    while current_date >= three_months_ago:\n",
    "        date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date -= timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "def get_dates():\n",
    "    existing = get_existing_files()\n",
    "    new_dates = last_three_months()\n",
    "    to_add = [item for item in new_dates if item not in existing]\n",
    "    return to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc50a38-70cd-49c3-aee0-9dd216ee6b18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to load wunderground data (without this it has no records to show)\n",
    "def render_page(url):\n",
    "    driver = webdriver.Chrome(service=s, options=options)    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    r = driver.page_source\n",
    "    driver.quit()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af48ef43-0154-48af-9b50-93efc0a230d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_transpose(data_list):\n",
    "    res_list = [[item.replace('%', '') for item in lst] for lst in data_list]\n",
    "    res_list = [[item.replace(u'\\xa0', u'') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°F','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°in','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°%','') for item in lst] for lst in res_list]\n",
    "    res_list = [[item.replace('°mph','') for item in lst] for lst in res_list]\n",
    "    final_list = [[item.replace('°','') for item in lst] for lst in res_list]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94df9798-cad6-428a-8316-b8314725df8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def set_schema(df):\n",
    "    # To Interger\n",
    "    df[[\"Temperature\",\"Dew_Point\", \"Humidity\",\"Wind_Speed\",\"Wind_Gust\"]] = df[[\"Temperature\",\"Dew_Point\", \"Humidity\",\"Wind_Speed\",\"Wind_Gust\"]].apply(pd.to_numeric)\n",
    "    df[['Pressure','Precipitation']] = df[['Pressure','Precipitation']].apply(pd.to_numeric)\n",
    "    # To DateTime\n",
    "    df['datetime'] = df['datetime'].apply(pd.to_datetime)\n",
    "    # To String\n",
    "    df[['Wind','Condition']] = df[['Wind','Condition']].applymap(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958101fd-76d4-4ad9-b70e-227e079bc881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-27\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-26\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-25\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-24\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-23\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-22\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-21\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-20\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-19\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-18\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-17\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-16\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-15\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-14\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-13\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-12\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-11\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-10\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-09\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-08\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-07\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-06\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-05\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-04\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-03\n",
      "https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/2023-02-02\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def scraper(page, dates):\n",
    "    '''function to scrape wunderground'''\n",
    "    for d in dates:\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        url = str(str(page) + str(d))\n",
    "        print(url)\n",
    "        r = render_page(url)\n",
    "\n",
    "        soup = BS(r, \"html.parser\")\n",
    "        container = soup.find('lib-city-history-observation')\n",
    "        check = container.find('tbody')\n",
    "\n",
    "        data = []\n",
    "        try:\n",
    "            for c in check.find_all('tr', class_='ng-star-inserted'):\n",
    "                for i in c.find_all('td', class_='ng-star-inserted'):\n",
    "                    trial = i.text\n",
    "                    trial = trial.strip('  ')\n",
    "                    data.append(trial)\n",
    "            \n",
    "            df_daily = []\n",
    "            cols = ['Time','Temperature','Dew_Point','Humidity','Wind','Wind_Speed','Wind_Gust','Pressure','Precipitation','Condition','Date']\n",
    "            for i in range(0,len(data),10):\n",
    "                snip_data = []\n",
    "                snip_data.append(data[i:i+10])\n",
    "                # Strip of Weird Characters\n",
    "                snip_data = list_transpose(snip_data)\n",
    "                snip_data[0].append(d)\n",
    "                df = pd.DataFrame(snip_data,columns=cols)\n",
    "                df['datetime'] = df['Date'] + ' ' + df['Time']\n",
    "                df = df.drop(['Date','Time'],axis=1) \n",
    "                # Set Schema\n",
    "                df = set_schema(df)\n",
    "                df_daily.append(df)\n",
    "\n",
    "            df_daily = pd.concat(df_daily)\n",
    "            num_rows_ingested = len(df_daily.index)\n",
    "            df_daily.to_parquet(f'/dbfs/mnt/taxi_etl/weather_data/NY_Weather{d}.parquet')\n",
    "\n",
    "            end_time = datetime.now()\n",
    "\n",
    "            # Logging JSON\n",
    "            file_name = f'NY_Weather{d}.parquet'\n",
    "            file_write_name = f'NY_Weather{d}.json'\n",
    "            start_time_str = start_time.isoformat()\n",
    "            end_time_str = end_time.isoformat()\n",
    "            data = {\"StartTime\":start_time_str,\"EndTime\":end_time_str,\"RowsIngested\":num_rows_ingested,\"FileName\":file_name}\n",
    "            data_json = json.dumps(data)\n",
    "            open(f'/dbfs/mnt/taxi_etl/weather_data_logs/{file_write_name}','w').write(data_json)\n",
    "            \n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "# Call Functions\n",
    "dates = get_dates()\n",
    "page = 'https://www.wunderground.com/history/daily/us/ny/new-york-city/KLGA/date/'\n",
    "\n",
    "scraper(page, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819824f7-e492-40e7-aabe-c9657d5da0ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh du -h /dbfs/mnt/taxi_etl/weather_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e135e588-4928-4d1c-b27d-43fb0d26cc37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh ls /dbfs/mnt/taxi_etl/weather_data/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed63171e-c590-48b9-9c50-f73f190b07bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make Directory\n",
    "dbutils.fs.mkdirs(\"/mnt/taxi_etl/weather_data/combined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6933f502-e8cb-4e7f-8398-4269b8d1cd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine weather data to check that schema is readable for all files\n",
    "def combine_weather_dfs():\n",
    "    path = '/mnt/taxi_etl/weather_data/*.parquet'\n",
    "    df = spark.read.option('inferSchema','true').parquet(path)\n",
    "    df = df.drop('__index_level_0__')\n",
    "    df = df.withColumnRenamed(\"Temperature\", \"temp(f)\")\\\n",
    "       .withColumnRenamed(\"Dew_Point\", \"dew_point(f)\")\\\n",
    "       .withColumnRenamed(\"Humidity\", \"humidity(%)\")\\\n",
    "       .withColumnRenamed(\"Wind\", \"wind_direction\")\\\n",
    "       .withColumnRenamed(\"Wind_Speed\", \"wind_speed(mph)\")\\\n",
    "       .withColumnRenamed(\"Wind_Gust\", \"wind_gust(mph)\")\\\n",
    "       .withColumnRenamed(\"Pressure\", \"pressure(inHg)\")\\\n",
    "       .withColumnRenamed(\"Precipitation\", \"precipitation(in)\")\\\n",
    "       .withColumnRenamed(\"Condition\", \"condition\")\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "combine_weather_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa23d482-7de1-4e6b-9e18-c13213c6b69c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### DeltaLake Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "effc35e0-e4d5-4830-9027-39a078ea99de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Convert parquet to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e710ba-3d53-49e4-975b-783e6bd0747e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read All Parquet Files from deltalake/parquet/\n",
    "path = '/mnt/taxi_etl/deltalake/parquet/*.parquet'\n",
    "df = spark.read.option('inferSchema','true').parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425b4d0a-8a60-4505-875b-3684bf95ed83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592a796a-389e-4c5f-af13-5027883b02f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save dataframe to delta\n",
    "delta_path = '/mnt/taxi_etl/deltalake/delta'\n",
    "df.coalesce(1).write.format('delta').mode('overwrite').save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f605eb-cf9e-4ea7-b772-53be545b6e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# See files\n",
    "dbutils.fs.ls(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0a662a-924c-4dec-8234-fa5398f7fc6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View each log individually\n",
    "log_directory = '/mnt/taxi_etl/deltalake/delta/_delta_log/'\n",
    "files = dbutils.fs.ls(log_directory)\n",
    "filenames = [file.name for file in files if file.name.endswith('.json')]\n",
    "for filename in filenames:\n",
    "    display(spark.read.text(log_directory+filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "849a319f-0f43-4d63-bff8-a32661212d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Analyze Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba8cc81-0b09-4d13-9336-00d525ca4253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Key metrics can be visualized and data can be examined in the Data Profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0698b1d-6ed5-4fb3-af95-35d1a38b6d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM delta.`/mnt/taxi_etl/deltalake/delta/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f03df201-4a6c-4d39-9e93-c91d279e3572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# More summary statistics can be found by using describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4aee42-1957-494c-8141-f1c0f791b84e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = '/mnt/taxi_etl/deltalake/delta/'\n",
    "df = spark.read.format(\"delta\").load(file_path)\n",
    "df.describe().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 465028284393408,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "taxi_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
